<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="https://tsairesearch.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tsairesearch.github.io/" rel="alternate" type="text/html" /><updated>2025-04-18T00:09:06-05:00</updated><id>https://tsairesearch.github.io/feed.xml</id><title type="html">The Scientific AI Research Group</title><entry><title type="html">Snap shots of recent research</title><link href="https://tsairesearch.github.io/recents" rel="alternate" type="text/html" title="Snap shots of recent research" /><published>2024-06-25T00:00:00-05:00</published><updated>2024-06-25T00:00:00-05:00</updated><id>https://tsairesearch.github.io/recents</id><content type="html" xml:base="https://tsairesearch.github.io/recents">&lt;!-- https://arxiv.org/abs/2406.10758 --&gt;
&lt;!-- https://drive.google.com/file/d/1VFbmtH5nxu9Bo8qzsd_YwPDfbOATYluO/view?usp=share_link --&gt;

&lt;h3&gt; Pursuit-evasion game of Reeds-Shepps cars using Deep Learning &lt;/h3&gt;

&lt;p&gt;Papers: &lt;a href=&quot;https://arxiv.org/abs/2406.10758&quot; title=&quot;Paper Link&quot;&gt;https://arxiv.org/abs/2406.10758 &lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Demos:&lt;/p&gt;

&lt;div style=&quot;grid-column: text;&quot;&gt;
&lt;figure&gt;
&lt;div style=&quot;display: inline; float: left; width: 50%; text-align: center&quot;&gt;
&lt;iframe frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://drive.google.com/file/d/18XFTo9Re3hmlpr1TBG0WfrqNE9YJ0pEH/preview&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;div style=&quot;display: inline; float: left; width: 50%; text-align: center&quot;&gt;
&lt;iframe frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;
https://drive.google.com/file/d/15jynOIUkxZIyjMTyilG4AzwQoSXewTx_/preview&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;/div&gt;

&lt;div style=&quot;column-grid: middle &quot;&gt;
&lt;figure&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;iframe width=&quot;360px&quot; height=&quot;240px&quot; allowfullscreen=&quot;true&quot; src=&quot;
https://drive.google.com/file/d/1VFbmtH5nxu9Bo8qzsd_YwPDfbOATYluO/preview&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/figure&gt;

&lt;/div&gt;

&lt;!-- &lt;div style=&quot;column-grid: middle &quot;&gt;
&lt;figure &gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;iframe width=&quot;720px&quot; height=&quot;480px&quot; allowfullscreen=&quot;true&quot; src=&quot;https://drive.google.com/file/d/1NW58DHqOJ5rYQoQ4lNo7VWmUIuB1BRvK/preview&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt; --&gt;

&lt;!-- https://drive.google.com/file/d/15xkqZbgdv5hzFde2pLgRe00EOEl4yish/preview --&gt;</content><author><name></name></author><category term="recents" /><summary type="html"></summary></entry><entry><title type="html">Sampling and Processing of Point Clouds</title><link href="https://tsairesearch.github.io/projects/rays" rel="alternate" type="text/html" title="Sampling and Processing of Point Clouds" /><published>2023-09-13T00:00:00-05:00</published><updated>2023-09-13T00:00:00-05:00</updated><id>https://tsairesearch.github.io/projects/rays</id><content type="html" xml:base="https://tsairesearch.github.io/projects/rays">&lt;!-- define the style for images container --&gt;
&lt;head&gt;
&lt;style&gt;
  div.container {
     display:inline-block;
   }

  html,body        {height: 100%;}

  .wrapper{width: 80%; max-width: 600px; height: 100%; margin: 0 auto; background: #CCC}

  .h_iframe{position: relative; padding-top: 56%;}

  .h_iframe iframe{position: absolute; top: 0; left: 0; width: 100%; height: 100%;}
&lt;/style&gt;
&lt;/head&gt;

&lt;body&gt;
&lt;d-front-matter&gt;
  &lt;script type=&quot;text/json&quot;&gt;{
  &quot;authors&quot;: [
    {
      &quot;author&quot;: &quot;Liangchen (Lewis) Liu&quot;,
      &quot;authorURL&quot;: &quot;https://lclewis.github.io&quot;,
      &quot;affiliation&quot;: &quot;Department of Mathematics, The University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;http://ma.utexas.edu/&quot;
    },
    {
      &quot;author&quot;: &quot;Louis Ly&quot;,
      &quot;authorURL&quot;: &quot;https://longlouisly.github.io/&quot;,
      &quot;affiliation&quot;: &quot;Oden Institute for Computational Engineering and Sciences, The University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.ices.utexas.edu&quot;
    },
    {
      &quot;author&quot;: &quot;Colin Macdonald&quot;,
      &quot;authorURL&quot;: &quot;https://www.math.ubc.ca/~cbm/&quot;,
      &quot;affiliation&quot;: &quot;Department of Mathematics, The University of British Columbia&quot;,
      &quot;affiliationURL&quot;: &quot;https://ubc.ca&quot;
    },

    {
      &quot;author&quot;: &quot;Yen-Hsi Richard Tsai&quot;,
      &quot;authorURL&quot;: &quot;https://www.ma.utexas.edu/users/ytsai/&quot;,
      &quot;affiliation&quot;: &quot;Oden Institute for Computational Engineering and Sciences, The University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    }
  ]
  }&lt;/script&gt;
&lt;/d-front-matter&gt;


&lt;d-title&gt;
 &lt;div style=&quot;grid-column: screen; text-align: center&quot;&gt;
  &lt;h1&gt;Sampling and Processing of Point Clouds&lt;/h1&gt;
    
    &lt;img style=&quot;display: block; max-width: 15%; margin-top: 3rem; margin-bottom: 3rem; margin-left: auto; margin-right: auto; border-radius: 5%&quot; alt=&quot;Sampling and Processing of Point Clouds&quot; src=&quot;https://tsairesearch.github.io//assets/images/rays/airplane_ray.png&quot; /&gt;
    
 &lt;/div&gt;


&lt;/d-title&gt;


&lt;d-article&gt;

&lt;p&gt; We propose a new framework for the sampling, compression, and analysis of distributions of point sets and other geometric objects embedded in Euclidean spaces. Our approach involves constructing a tensor called the RaySense sketch, which captures nearest neighbors from the underlying geometry of points along a set of rays. We explore various operations that can be performed on the RaySense sketch, leading to different properties and potential applications. Statistical information about the data set can be extracted from the sketch, independent of the ray set. Line integrals on point sets can be efficiently computed using the sketch. We also present several examples illustrating applications of the proposed strategy in practical scenarios. &lt;/p&gt;

&lt;h2&gt;  Visualizations of Method &lt;/h2&gt;

&lt;h3&gt; Sampling Visualization with rays and Features &lt;/h3&gt;
&lt;div style=&quot;grid-column: screen &quot;&gt;

&lt;!-- &lt;figure&gt;
&lt;div style=&quot;display: block; width: 80%; text-align: center&quot;&gt;
&lt;iframe width=&quot;1296px&quot; height=&quot;850px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io/assets/images/rays/grid.png&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt; --&gt;


&lt;figure&gt;
&lt;div style=&quot;display: block; width: 80%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
&lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/grid.png&quot; width=&quot;338px&quot; height=&quot;648px&quot; /&gt;
&lt;/div&gt;

&lt;!-- &lt;div class=&quot;wrapper&quot;&gt;
    &lt;div class=&quot;h_iframe&quot;&gt;
      &lt;iframe height=&quot;4&quot; width=&quot;8&quot; src=&quot;https://tsairesearch.github.io/assets/images/rays/grid.png&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;
    &lt;/div&gt;
&lt;/div&gt; --&gt;
&lt;!-- &lt;div class=&quot;container&quot;&gt;
    &lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/grid.png&quot; height=&quot;1024&quot; width=&quot;768&quot;/&gt;
&lt;/div&gt; --&gt;

&lt;figcaption&gt;
&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
(Row 1) Visualization of a two rays (black) through points sampled from various objects (gray). Closest point pairs are shown in green and red. (Row 2) Plot of distance from points along the ray to the corresponding closest points on the object. (Rows 3-5) The $x$, $y$, and $z$ coordinates of the closest points to the ray.
&lt;/div&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;

&lt;h3&gt; Visualization of Voronoi Cell with Rays &lt;/h3&gt;
&lt;div style=&quot;grid-column: screen &quot;&gt;

&lt;figure&gt;
&lt;div style=&quot;display: block; width: 100%; text-align: center&quot;&gt;
&lt;iframe width=&quot;800px&quot; height=&quot;365px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io/assets/images/rays/voronoi.png&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;figcaption&gt;
&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
Two rays (black)
sense nearest neighbors of the point set (blue). Singular
points, such as the tip of the tail, have larger Voronoi cells
(dashed lines) and are more likely to be sampled. Closest
point pairs are shown in green and red.

&lt;/div&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;

&lt;h3&gt; Visualization of Sampling Frequencies&lt;/h3&gt;
&lt;div style=&quot;grid-column: screen &quot;&gt;

&lt;figure&gt;
&lt;div style=&quot;display: block; width: 50%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
&lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/airplane_side.png&quot; width=&quot;210px&quot; height=&quot;350px&quot; /&gt;
&lt;/div&gt;

&lt;figcaption&gt;
&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
Rays are more likely to sample salient features in the point cloud. Larger points are repeated more often.
We can control the number of points by increasing the number of rays. Each ray contains 30 sample points.  
&lt;/div&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;



&lt;h2&gt; Downstream Applications &lt;/h2&gt;


  &lt;h3&gt; Salient Points Detection &lt;/h3&gt;

    &lt;figure class=&quot;half&quot; style=&quot;display:flex&quot;&gt;

      &lt;div class=&quot;container&quot;&gt;

        &lt;div style=&quot;display: block; width: 90%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
        &lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/MNIST_Salient_RaySense_top10_01cube-202210-noFreq.png&quot; width=&quot;210px&quot; height=&quot;350px&quot; /&gt;
        &lt;/div&gt;


        &lt;figcaption&gt;
        &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
          RaySense Sampling with Top 10 Frequencies

        &lt;/div&gt;
        &lt;/figcaption&gt;
      &lt;/div&gt;

      &lt;div class=&quot;container&quot;&gt;
        &lt;div style=&quot;display: block; width: 90%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
        &lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/MNIST_random_10-202210-noFreq.png&quot; width=&quot;210px&quot; height=&quot;350px&quot; /&gt;
        &lt;/div&gt;

        &lt;figcaption&gt;
        &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
          Uniformly Random Sampling
          &lt;/div&gt;
        &lt;/figcaption&gt;
    &lt;/div&gt;
    &lt;/figure&gt;

        &lt;figure&gt;
          &lt;figcaption&gt;
          &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 100%; text-align: center&quot;&gt;
          Comparison of RaySense sampled points and points from uniformly random sampling of the MNIST dataset. RaySense sampled points are those with the highest RaySense sampling frequencies for each class. These points correspond to digits written using unusual strokes, hence giving a notion of salient points of the dataset.
          &lt;/div&gt;
          &lt;/figcaption&gt;
        &lt;/figure&gt;



    &lt;h3&gt; Radon Transform and Reconstruction&lt;/h3&gt;

        &lt;figure class=&quot;half&quot; style=&quot;display:flex&quot;&gt;

          &lt;div class=&quot;container&quot;&gt;

            &lt;div style=&quot;display: block; width: 90%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
            &lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/point_cloud_and_density.png&quot; width=&quot;210px&quot; height=&quot;160px&quot; /&gt;
            &lt;/div&gt;


            &lt;figcaption&gt;
            &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
              Underlying density and point sets
            &lt;/div&gt;
            &lt;/figcaption&gt;
          &lt;/div&gt;

          &lt;div class=&quot;container&quot;&gt;
            &lt;div style=&quot;display: block; width: 90%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
            &lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/example_M21.png&quot; width=&quot;210px&quot; height=&quot;160px&quot; /&gt;
            &lt;/div&gt;

            &lt;figcaption&gt;
            &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
              RaySensed points from uniform lines
              &lt;/div&gt;
            &lt;/figcaption&gt;
        &lt;/div&gt;

        &lt;div class=&quot;container&quot;&gt;
          &lt;div style=&quot;display: block; width: 90%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
          &lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/sonogram_92441_M101_K64_dt1.png&quot; width=&quot;210px&quot; height=&quot;160px&quot; /&gt;
          &lt;/div&gt;

          &lt;figcaption&gt;
          &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
            Integral transformed / spectrum
            &lt;/div&gt;
          &lt;/figcaption&gt;
        &lt;/div&gt;

        &lt;div class=&quot;container&quot;&gt;
          &lt;div style=&quot;display: block; width: 90%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
          &lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/fbp_92441_M101_K64_dt1_trim.png&quot; width=&quot;210px&quot; height=&quot;160px&quot; /&gt;
          &lt;/div&gt;

          &lt;figcaption&gt;
          &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
            Reconstruction results
            &lt;/div&gt;
          &lt;/figcaption&gt;
        &lt;/div&gt;
        &lt;/figure&gt;

            &lt;figure&gt;
              &lt;figcaption&gt;
              &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 100%; text-align: center&quot;&gt;
            Approximate Radon transform computed with RaySense from point cloud data (a)–(c) and filtered back projection reconstruction (d). Our theoretical analysis also suggests the line integral error from RaySense approximation of an $N$-size point cloud in $\mathbb R^d$ converge in a rate of $\mathcal O\left(N^{-\frac{1}d}\right)$.
              &lt;/div&gt;
              &lt;/figcaption&gt;
            &lt;/figure&gt;


&lt;h3&gt; Classification using Deep Neural Networks &lt;/h3&gt;

&lt;div style=&quot;grid-column: screen &quot;&gt;
&lt;figure&gt;
&lt;div style=&quot;display: block; width: 100%; text-align: center&quot;&gt;
&lt;iframe width=&quot;900px&quot; height=&quot;250px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io/assets/images/rays/architecture.png&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;figcaption&gt;
&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
Neural network architecture for point cloud classification based on ray signatures.

&lt;/div&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
&lt;div style=&quot;display: block; width: 100%; text-align: center&quot;&gt;
&lt;iframe width=&quot;600px&quot; height=&quot;320px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io/assets/images/rays/missing_data.png&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;figcaption&gt;
&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
Comparison of our neural network (RayNN) against other prominent methods on ModelNet40 dataset with missing data.
Our model is robust, even when the data consists of only a few number of points.

&lt;/div&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;



&lt;/div&gt;




&lt;div style=&quot;grid-column: screen; margin-left: auto; margin-right: auto; text-align: center &quot;&gt;

&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 80%; text-align: center&quot;&gt;
&lt;figure&gt;
&lt;figcaption&gt;
&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 100%; text-align: center&quot;&gt;
Comparison of our deep neural network model against PointNet on ModelNet point cloud classification dataset.
&lt;/div&gt;
&lt;/figcaption&gt;


&lt;table&gt;
  &lt;tr&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;Model&lt;/th&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;ModelNet10 (2048)&lt;/th&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;ModelNet40 (2048)&lt;/th&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;ModelNet40 (1024)&lt;/th&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;ModelNet40 (4096)&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;PointNet (paper)&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;-&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;-&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;89.2%&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;-&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;PointNet.pytorch&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;92.07% (on 256 pts)&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;85.3% (on 1024 pts)&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;87.52%&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;85.4%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;Ours&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;95.04%&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;90.03%&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;90.6%&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;90.44%&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;/figure&gt;
&lt;/div&gt;

&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
&lt;figure&gt;
&lt;figcaption&gt;
&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 100%; text-align: center&quot;&gt;
Sensitivity of the neural network models to the input size. Here, $m$ is the number of rays, $N^\ast$ is the number of points used, and $N$ is the total number of points in the original point set.
&lt;/div&gt;
&lt;/figcaption&gt;


&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;Our model on ModelNet10 (N=2048)&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;$m/32$&lt;/td&gt;
    &lt;td&gt;100%&lt;/td&gt;
    &lt;td&gt;50%&lt;/td&gt;
    &lt;td&gt;25%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;$N^\ast/N$&lt;/td&gt;
    &lt;td&gt;15.38%&lt;/td&gt;
    &lt;td&gt;8.59%&lt;/td&gt;
    &lt;td&gt;4.59%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Performance/Accuracy&lt;/td&gt;
    &lt;td&gt;94.60%&lt;/td&gt;
    &lt;td&gt;95.04%&lt;/td&gt;
    &lt;td&gt;94.60%&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;Our model on ModelNet40 (N=1024)&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;$m/32$&lt;/td&gt;
    &lt;td&gt;100%&lt;/td&gt;
    &lt;td&gt;50%&lt;/td&gt;
    &lt;td&gt;25%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;$N^\ast/N$&lt;/td&gt;
    &lt;td&gt;25.10%&lt;/td&gt;
    &lt;td&gt;14.75%&lt;/td&gt;
    &lt;td&gt;8.17%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Performance/Accuracy&lt;/td&gt;
    &lt;td&gt;90.56%&lt;/td&gt;
    &lt;td&gt;90.60%&lt;/td&gt;
    &lt;td&gt;89.82%&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;PointNet on ModelNet40 (N=1024)&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;$N^\ast/N$&lt;/td&gt;
    &lt;td&gt;100%&lt;/td&gt;
    &lt;td&gt;50%&lt;/td&gt;
    &lt;td&gt;12.5%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Performance/Accuracy&lt;/td&gt;
    &lt;td&gt;89.2%&lt;/td&gt;
    &lt;td&gt;86.8%&lt;/td&gt;
    &lt;td&gt;69%&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;/figure&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;h2&gt; Publications &lt;/h2&gt;

&lt;p&gt;
&lt;li&gt;  
Liu, L., Ly, L., Macdonald, C. &amp;amp; Tsai, R. (2023) Nearest Neighbor Sampling of Point Sets using Rays. Communication on Applied Mathematics and Computation (CAMC), Focused Issue in Honor of Prof. Stanley Osher on the Occasion of His 80th Birthday. Accepted.
&lt;/li&gt;

&lt;/p&gt;

&lt;/d-article&gt;
&lt;/body&gt;</content><author><name></name></author><category term="projects" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/rays/airplane_ray.png" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/rays/airplane_ray.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Optimizing Sensor Network Design for Multiple Coverage</title><link href="https://tsairesearch.github.io/projects/sensor" rel="alternate" type="text/html" title="Optimizing Sensor Network Design for Multiple Coverage" /><published>2023-09-01T00:00:00-05:00</published><updated>2023-09-01T00:00:00-05:00</updated><id>https://tsairesearch.github.io/projects/sensor</id><content type="html" xml:base="https://tsairesearch.github.io/projects/sensor">&lt;head&gt;
&lt;style&gt;
  div.container {
     display:inline-block;
   }

  html,body        {height: 100%;}

  .wrapper{width: 80%; max-width: 600px; height: 100%; margin: 0 auto; background: #CCC}

  .h_iframe{position: relative; padding-top: 56%;}

  .h_iframe iframe{position: absolute; top: 0; left: 0; width: 100%; height: 100%;}
&lt;!-- &lt;/style&gt;
&lt;style&gt; --&gt;
  .bordered {
        border: 1px solid black; /* 1px solid black border */
        padding: 10px; /* Optional: Add padding inside the border */
        margin: 10px; /* Optional: Add margin outside the border */
        background-color: #f0f0f0;
    }
    /* CSS to style the pre tag */
    pre {
        font-family: Courier, monospace;
        background-color: #f8f8f8;
        padding: 10px;
        border: 1px solid #ddd;
        white-space: pre-wrap; /* Preserve line breaks */
    }
    /* Add padding to the body */
    body {
        padding-left: 20px;
        padding-right: 20px;
    }
    .container {
        display: flex;
    }
&lt;/style&gt;
&lt;/head&gt;

&lt;body&gt;
&lt;d-front-matter&gt;
  &lt;script type=&quot;text/json&quot;&gt;{
  &quot;authors&quot;: [
    {
      &quot;author&quot;: &quot;Lukas Taus&quot;,
      &quot;authorURL&quot;: &quot;https://oden.utexas.edu/people/directory/Lukas-Taus&quot;,
      &quot;affiliation&quot;: &quot;Oden Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    },
    {
      &quot;author&quot;: &quot;Yen-Hsi Richard Tsai&quot;,
      &quot;authorURL&quot;: &quot;https://www.ma.utexas.edu/users/ytsai/&quot;,
      &quot;affiliation&quot;: &quot;Oden Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    }
  ]
  }&lt;/script&gt;
&lt;/d-front-matter&gt;

&lt;d-title&gt;
 &lt;div style=&quot;grid-column: screen; text-align: center&quot;&gt;
  &lt;h1&gt;Optimizing Sensor Network Design for Multiple Coverage&lt;/h1&gt;
    
    &lt;center&gt;
    &lt;video width=&quot;1000&quot; height=&quot;500&quot; controls=&quot;&quot;&gt;
        &lt;source src=&quot;/assets/images/sensors/ParGreedy_Video.mp4&quot; type=&quot;video/mp4&quot; /&gt;
        Your browser does not support the video tag.
    &lt;/video&gt;
    &lt;/center&gt;
    
 &lt;/div&gt;

&lt;/d-title&gt;


&lt;d-article&gt;

&lt;div class=&quot;bordered&quot;&gt;
    &lt;h2 style=&quot;color: black&quot;&gt;Abstract&lt;/h2&gt;
    &lt;p&gt;Sensor placement optimization methods have been studied extensively. They can be applied to a wide range of applications, including surveillance of known environments, optimal locations for 5G towers, and placement of missile defense systems. However, few works explore the robustness and efficiency of the resulting sensor network concerning sensor failure or adversarial attacks. This paper addresses this issue by optimizing for the least number of sensors to achieve multiple coverage of non-simply connected domains by a prescribed number of sensors. We introduce a new objective function for the greedy (next-best-view) algorithm to design efficient and robust sensor networks and derive theoretical bounds on the network's optimality.
    We further introduce a Deep Learning model to accelerate the algorithm for near real-time computations. The Deep Learning model requires the generation of training examples. Correspondingly, we show that understanding the geometric properties of the training data set provides important insights into the performance and training process of deep learning techniques. Finally, we demonstrate that a simple parallel version of the greedy approach using a simpler objective can be highly competitive.&lt;/p&gt;
&lt;/div&gt;


&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the multi-coverage sensor placement problem the goal is to compute optimal sensor locations such that the majority of the free space is observed by at least \( k \) sensors. For this we assume that an environment \( \Omega \) consists of regions occluded by obstacles \( \Omega_\text{obs} \) and free space \( \Omega_\text{free} \).&lt;/p&gt;

&lt;div class=&quot;container&quot;&gt;
    &lt;div&gt;
        &lt;p&gt;A point \( x \in \Omega_\text{free} \) is visible to another point \( y \in \Omega_\text{free} \) if there is an unobstructed straight line between them.&lt;/p&gt;
        &lt;p&gt;\[ \mathbf{x} \overset{\text{vis}}{\sim} \mathbf{y} \iff  \forall t \in [0,1] \text{: } t\mathbf{x} + (1-t)\mathbf{y} \in \Omega_\text{free}. \]&lt;/p&gt;

        &lt;p&gt;The order of visibility function \( \mathcal{O}_\text{vis} \) returns the number of sensors in a set of sensor \( P \) that observe a given point.&lt;/p&gt;
        &lt;p&gt;\[ \mathcal{O}_\text{vis}(\mathbf{y}; P) = \sum_{x \in P} \phi_{\mathbf{x}}(\mathbf{y}) \]&lt;/p&gt;
        &lt;p&gt;where&lt;/p&gt;
        &lt;p&gt;\[ \phi_{\mathbf{x}}(\mathbf{y}) = \begin{cases} 1, &amp;amp; \text{if } \mathbf{x} \overset{\text{vis}}{\sim} \mathbf{y},\\ 0, &amp;amp; \text{else.} \end{cases} \]&lt;/p&gt;
        &lt;p&gt;The multi-coverage sensor placement problem can then be described as&lt;/p&gt;
        &lt;div class=&quot;bordered&quot;&gt;
            &lt;h4&gt;Multiple coverage optimization:&lt;/h4&gt;
            &lt;p&gt;Find the minimal set of sensors \( P = \{x_1,...,x_n\} \) such that&lt;/p&gt;
            &lt;p&gt;\[ \text{Vol} \left( \left\{y\in \Omega \vert~ \mathcal{O}_\text{vis}(y; P) \geq k \right\} \right) \le (1 - \delta) \text{Vol}(\Omega_{\text{free}}), \]&lt;/p&gt;
            &lt;p&gt;for some given threshold \( \delta \in (0,1). \) &lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
        &lt;img src=&quot;/assets/images/sensors/OvisK.png&quot; align=&quot;right&quot; width=&quot;700&quot; height=&quot;525&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;h2 style=&quot;color: black&quot;&gt;Greedy Algorithm&lt;/h2&gt;
&lt;p&gt;Even the single coverage problem is known to be NP-complete for general environments. To address the challenge of efficient computation of sensor location we relax the problem using the next-best-view principle where sequences of sensors are generated instead of sets. This leads to the development of the \( \epsilon \)-greedy algorithm.&lt;/p&gt;
&lt;pre&gt;
    def eps_greedy_alg(eps, k):
        P = list()
        while Vol(O_vis(P) &amp;gt;= k) &amp;lt;= delta:
            G = compute_objective_G(P, k)
            M = max(G)
            feasible = [x for x in PossibleLocations if G(x) &amp;gt;= (1-eps)*M]
            x = feasible[randint(0, len(feasible))]
            P.append(x)
        return P&lt;/pre&gt;
&lt;p&gt;This algorithm chooses the next sensor by (somewhat) greedily maximizing a objective function (using function  &lt;span style=&quot;font-family: monospace;&quot;&gt;compute_objective_G&lt;/span&gt;). In our case the objective function return the gained volume of regions of order up to target order \( k \) by placing a new sensor in a given location.&lt;/p&gt;
&lt;p&gt;\[ \mathcal{G}_k(x, P) = \sum_{i=1}^k w_i \int_{\Omega_\text{free}} 1_{\{\mathcal{O}_{\text{vis}}(\mathbf{z}, P \cup \{x\}) \geq i\}} - 1_{\{\mathcal{O}_{\text{vis}}(\mathbf{z}, P) \geq i\}} d\mathbf{z}. \]&lt;/p&gt;
&lt;p&gt;Here the integrals descirbe the gain in the volume of regions of order at least \( i \) which are weighted by weights \( w_i \). The weights describe the priority of different orders of visiblity in the algorithm.&lt;/p&gt;
&lt;p&gt;To measure the quality of a set of sensors we use a function \( f_k \) closely realted to the objective function \( \mathcal{G}_k \).&lt;/p&gt;
&lt;p&gt;\[ f_k(P) = \sum_{i=1}^k w_i \int_{\Omega_\text{free}} 1_{\{\mathcal{O}_{\text{vis}}(\mathbf{z}, P) \geq i\}} d\mathbf{z}. \]&lt;/p&gt;
&lt;p&gt;It describes a weighted sum of volumes of regions with specified minimal achieved order. If a region is not observed by any sensor it assigns value \( 0 \). If a region is observed by at least one sensor it assigns value \( w_1 \), if observed by at least two sensors the assigned value is \( w_1 + w_2 \) and so on. This allows us to measure the value of partial coverage and we are able to analyze the quality of the sensor set produced by the greedy algorithm.&lt;/p&gt;
&lt;div class=&quot;bordered&quot;&gt;
    &lt;h4&gt;Effectivity guarantee&lt;/h4&gt;
    &lt;p&gt;Let \( P_n = \{x_1,...,x_n\} \) be the set of $n$ sensors placed according to the greedy algorithm using \( \mathcal{G}_k \) and parameter \( \epsilon \in [0,1) \).&lt;/p&gt;
    &lt;p&gt;\[ P_l^* = \{x_1^*,...,x_l^*\}\in \underset{P \subseteq \Omega_\text{free}, \vert P \vert = l}{\operatorname{argmax}}  f_k(P) \implies f_k(P_n) \geq \left( 1 -  e^{-(1-\epsilon)\frac{n}{l}} \right) f_K(P_l^*). \]&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Also for empirical examples the algorithm produces promising results. In the example shown 10 sensor are sufficient to cover 45% of \(\Omega_\text{free}\) with order 3 visiblity (left). After 24 sensors are placed 90% of the free space is observed by at least 3 sensors and the greedy algorithm terminates (right).&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&quot;/assets/images/sensors/Greedy_Results.jpg&quot; width=&quot;700&quot; height=&quot;343&quot; align=&quot;left&quot; /&gt;
&lt;/center&gt;
&lt;h2&gt;Parallel Greedy Algorithm&lt;/h2&gt;
&lt;p&gt;Another approach we looked at was to split the multi-coverage problem into multiple single coverage problems. Solving the &lt;a href=&quot;https://arxiv.org/abs/2010.09001&quot;&gt;single coverage problem&lt;/a&gt; using the greedy algorithm using objective function \( G_1 \) has been expensively studied in the past and promises to be effective. Since practice we want to avoid sensors being placed too close or at the same location, we place the first sensor in each run randomly to introduce variability in the sensor network.&lt;/p&gt;
&lt;pre&gt;
    def par_greedy_alg(eps, k):
        P = list()
        for i in range(k):
            S = eps_greedy_alg(eps, 1)
            P += S
        return P&lt;/pre&gt;
&lt;p&gt;Note that the calculations inside the for loop are independent of eachother and can therefore be performed in parallel which significantly increases the speed. Here 9 sensors were needed to observe 43% of the \( \Omega_\text{free} \) with at least 3 sensors which is slightly worse that the result of the order 3 greedy algorithm. However the termination criterion of 90% observed by at least 3 sensors is reached using only 18 sensors.&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&quot;/assets/images/sensors/RoundRobin_Results.jpg&quot; width=&quot;700&quot; height=&quot;343&quot; align=&quot;left&quot; /&gt;
&lt;/center&gt;
&lt;h2&gt;Learning the Objective Function using Deep Learning&lt;/h2&gt;
&lt;p&gt;In the algorithms discussed above the evaluation of the objective function \( \mathcal{G}_k \) and \( \mathcal{G}_k \) is still computationally expensive. We use deep learning techniques to compute approximations of $\( \mathcal{G}_k \) which significantly imporves the speed of the algorithm once trained. In this approach we use the &lt;a href=&quot;https://arxiv.org/abs/1611.09326&quot;&gt;TiraFL&lt;/a&gt; network architecture to approximate the objective function \( \mathcal{G}_k \).&lt;/p&gt;
&lt;p&gt;\[ \mathcal{G}^\theta_k(x, P, \Omega_\text{free}) \approx G_k(x, P) \]&lt;/p&gt;
&lt;p&gt;The approximation of \( \mathcal{G}_1 \) has been discussed in &lt;a href=&quot;https://arxiv.org/abs/2010.09001&quot;&gt;&quot;Visibility Optimization for Surveillance-Evasion Games&quot;&lt;/a&gt;. To compare the different methods we used 50 sample maps generated by assigning random heights to random crops of &lt;a href=&quot;https://www.cs.toronto.edu/~vmnih/docs/Mnih_Volodymyr_PhD_Thesis.pdf&quot;&gt;Massachusetts building footprints&lt;/a&gt;.&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&quot;/assets/images/sensors/Boxplot_90.png&quot; width=&quot;700&quot; height=&quot;343&quot; align=&quot;left&quot; /&gt;
&lt;/center&gt;
&lt;p&gt;The boxplots illustrate the number of sensor needed to reach the threshold of 90% order 3 coverage. Both the greedy and parallel greedy algorithm are shown to be very effective. The network approximation, while not as effective, still consistently produces good results. As reference we also show the results for random placement of sensors.&lt;/p&gt;


&lt;h3&gt; Publications &lt;/h3&gt;

&lt;p&gt;
&lt;li&gt;  
Taus, L., &amp;amp; Tsai, R. (2023) Efficient and robust Sensor Placement in Complex Environments. Preprint.
&lt;/li&gt;

&lt;/p&gt;



&lt;/d-article&gt;
&lt;/body&gt;

&lt;!-- &lt;h3&gt; Simulation &lt;/h3&gt;


&lt;div style=&quot;column-grid: middle &quot;&gt;

&lt;figure &gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;iframe src=&quot;https://drive.google.com/file/d/12LGm468ZypvgAKlA1CVIV5Qu1ziiytb-/preview&quot; width=&quot;640&quot; height=&quot;480&quot; allow=&quot;autoplay&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 &lt;figcaption &gt;
   [missing]
 &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt; --&gt;</content><author><name></name></author><category term="projects" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/sensors/OvisK.png" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/sensors/OvisK.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Impact of the Extrinsic Geometry on Linear Regression</title><link href="https://tsairesearch.github.io/projects/extrinsic_geom" rel="alternate" type="text/html" title="Impact of the Extrinsic Geometry on Linear Regression" /><published>2023-06-30T00:00:00-05:00</published><updated>2023-06-30T00:00:00-05:00</updated><id>https://tsairesearch.github.io/projects/extrinsic_geom</id><content type="html" xml:base="https://tsairesearch.github.io/projects/extrinsic_geom">&lt;head&gt;
&lt;style&gt;
  div.container {
     display:inline-block;
   }

  html,body        {height: 100%;}

  .wrapper{width: 80%; max-width: 600px; height: 100%; margin: 0 auto; background: #CCC}

  .h_iframe{position: relative; padding-top: 56%;}

  .h_iframe iframe{position: absolute; top: 0; left: 0; width: 100%; height: 100%;}
&lt;/style&gt;
&lt;/head&gt;

&lt;d-front-matter&gt;

  &lt;script type=&quot;text/json&quot;&gt;{
  &quot;authors&quot;: [
    {
      &quot;author&quot;: &quot;Liangchen (Lewis) Liu&quot;,
      &quot;authorURL&quot;: &quot;https://lclewis.github.io/&quot;,
      &quot;affiliation&quot;: &quot;Department of Mathematics, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.ma.utexas.edu/&quot;
    },
    {
      &quot;author&quot;: &quot;Juncai He&quot;,
      &quot;authorURL&quot;: &quot;https://juncaihe.github.io/&quot;,
      &quot;affiliation&quot;: &quot;Applied Mathematics and Computational Sciences, The King Abdullah University of Science and Technology (KAUST)&quot;,
      &quot;affiliationURL&quot;: &quot;https://cemse.kaust.edu.sa/amcs&quot;
    },
    {
      &quot;author&quot;: &quot;Yen-Hsi Richard Tsai&quot;,
      &quot;authorURL&quot;: &quot;https://www.ma.utexas.edu/users/ytsai/&quot;,
      &quot;affiliation&quot;: &quot;Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.ices.utexas.edu&quot;
    }

  ]
  }&lt;/script&gt;
&lt;/d-front-matter&gt;

&lt;d-title&gt;

&lt;div style=&quot;grid-column: screen; text-align: center&quot;&gt;
  &lt;h1&gt;Impact of the Extrinsic Geometry on Linear Regression&lt;/h1&gt;
    
    &lt;img style=&quot;display: block; max-width: 15%; margin-top: 3rem; margin-bottom: 3rem; margin-left: auto; margin-right: auto; border-radius: 5%&quot; alt=&quot;Impact of the Extrinsic Geometry on Linear Regression&quot; src=&quot;https://tsairesearch.github.io//assets/images/extrinsic_geom/manifold_demo_3.pdf&quot; /&gt;
    
 &lt;/div&gt;
&lt;/d-title&gt;

&lt;d-article&gt;

&lt;p&gt;
In this paper, we study linear regression applied to data structured on a manifold. We assume that the data manifold is smooth and is embedded in a Euclidean space, and our objective is to reveal the impact of the data manifold's extrinsic geometry on the regression. Specifically, we analyze the impact of the manifold's curvatures (or higher order nonlinearity in the parameterization when the curvatures are locally zero) on the uniqueness of the regression solution. Our findings suggest that the corresponding linear regression does not have a unique solution when the manifold is flat. Otherwise, the manifold's curvature (or higher order nonlinearity in the embedding) may contribute significantly, particularly in the solution associated with the normal directions of the manifold. Our findings thus reveal the role of data manifold geometry in ensuring the stability of regression models for out-of-distribution inferences.
&lt;/p&gt;

&lt;h3&gt; Main results &lt;/h3&gt;
We investigate the local linear regression on smooth function $g(\mathbf{x})$ with data points sampled from different smooth manifolds $M\subset \mathbb R^d$.
&lt;p&gt;
&lt;ol&gt;
  &lt;li&gt; When $M$ is given by $M=(x, y=kx^2)\subset \mathbb R^2$, where $k$ characterizes the curvature, the linear regression has leading order solutions: $w_x = \frac{\partial g}{\partial x}$, $w_y = \frac{\partial g}{\partial y} + \frac{1}{2k}\frac{\partial^2 g}{\partial x^2}$. This implies a direct impact of the manifold's curvature on the linear regression. Numerical verifications of this result are provided below:

  &lt;div style=&quot;grid-column: screen &quot;&gt;

  &lt;figure&gt;
  &lt;div style=&quot;display: block; width: 100%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
  &lt;iframe width=&quot;400px&quot; height=&quot;290px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io/assets/images/extrinsic_geom/2d_regression.pdf&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
  &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
  Numerical simulations with different $k$.
  &lt;/div&gt;
  &lt;/figcaption&gt;
  &lt;/figure&gt;
  &lt;/div&gt;


&lt;/li&gt;
  &lt;li&gt; For any hypersurface $M\subset\mathbb R^d$, one can obtain a quadratic approximation locally. Then a generalized solution formula for the local linear regression can be explicitly obtained:
  $$ \begin{cases}
 w_{x_i} = \frac{\partial g}{\partial x_i},\\
	w_y = \frac{\partial g}{\partial y} + \frac{1}{2}\frac{\displaystyle\sum_{i=1}^{d-1}k_i \frac{\partial^2 g}{\partial x_i^2}}{\displaystyle\sum_{i=1}^{d-1}k_i^2}. \\
\end{cases} $$
&lt;/li&gt;

  &lt;li&gt;
  Data in practice usually contains noises. We show that a suitable scale of Gaussian noise $\sim\mathcal N(0, \sigma^2)$ can regularize the behavior caused by the problematic curvature. This balancing effect is demonstrated in the formula:
  $$ w_y = \frac{\partial g}{\partial y} + \frac{1}{2}\frac{k}{k^2+\frac{45}4\sigma^2}\frac{\partial^2 g}{\partial x^2},$$
  and verified in the numerical experiment:
  &lt;figure&gt;
  &lt;div style=&quot;display: block; width: 100%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
  &lt;iframe width=&quot;400px&quot; height=&quot;290px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io/assets/images/extrinsic_geom/2d_regression_noise.pdf&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
  &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
  Numerical simulations with different scale of noise $\sigma$.
  &lt;/div&gt;
  &lt;/figcaption&gt;
  &lt;/figure&gt;
  The intuition behind is that the noise blurs the problematic structure of the manifold.
  &lt;/li&gt;

  &lt;li&gt; For manifold with codimension $&amp;gt;1$, we perform numerical experiments to demonstrate the importance of the awareness of extrinsic dimension and embedding dimension. These characteristics crucially affect the well-posedness of the regression algorithm. See paper for details.
  &lt;/li&gt;

&lt;/ol&gt;
&lt;/p&gt;

&lt;h3&gt; Publications &lt;/h3&gt;

&lt;p&gt;
&lt;li&gt; Liu, L., He, J., &amp;amp; Tsai, R. (2023) Linear Regression on Manifold Structured Data: the Impact of Extrinsic Geometry on Solutions. Topology, Algebra and Geometry in Machine Learning workshop, International Conference of Machine Learning
&lt;/li&gt;
&lt;/p&gt;


&lt;/d-article&gt;</content><author><name></name></author><category term="projects" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/extrinsic_geom/manifold_demo_3.pdf" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/extrinsic_geom/manifold_demo_3.pdf" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Previous Photos</title><link href="https://tsairesearch.github.io/photos" rel="alternate" type="text/html" title="Previous Photos" /><published>2023-02-20T00:00:00-06:00</published><updated>2023-02-20T00:00:00-06:00</updated><id>https://tsairesearch.github.io/photos</id><content type="html" xml:base="https://tsairesearch.github.io/photos">&lt;h3&gt; 2023 &lt;/h3&gt;
&lt;figure&gt;

&lt;img style=&quot;display: block; max-width: 100%; margin-top: 3rem; margin-bottom: 1rem; margin-left: auto; margin-right: auto&quot; alt=&quot;Previous Photos&quot; src=&quot;https://tsairesearch.github.io/assets/images/old_profile/group_photo-2023.jpg&quot; /&gt;
&lt;/figure&gt;

&lt;h3&gt; 2021 &lt;/h3&gt;
&lt;figure&gt;
&lt;img style=&quot;display: block; max-width: 100%; margin-top: 3rem; margin-bottom: 1rem; margin-left: auto; margin-right: auto&quot; alt=&quot;Previous Photos&quot; src=&quot;https://tsairesearch.github.io/assets/images/old_profile/group_photo-2021.png&quot; /&gt;
&lt;/figure&gt;

&lt;h3&gt; 2019 &lt;/h3&gt;
&lt;figure&gt;
&lt;img style=&quot;display: block; max-width: 100%; margin-top: 3rem; margin-bottom: 1rem; margin-left: auto; margin-right: auto&quot; alt=&quot;Previous Photos&quot; src=&quot;https://tsairesearch.github.io/assets/images/old_profile/group_photo-2019.jpg&quot; /&gt;
&lt;/figure&gt;</content><author><name></name></author><category term="photos" /><summary type="html">2023</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/github_avatar.jpg" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/github_avatar.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Numerical Wave Propagation Aided by Deep Learning</title><link href="https://tsairesearch.github.io/projects/wavelnn" rel="alternate" type="text/html" title="Numerical Wave Propagation Aided by Deep Learning" /><published>2021-08-16T00:00:00-05:00</published><updated>2021-08-16T00:00:00-05:00</updated><id>https://tsairesearch.github.io/projects/wavelnn</id><content type="html" xml:base="https://tsairesearch.github.io/projects/wavelnn">&lt;d-front-matter&gt;

  &lt;script type=&quot;text/json&quot;&gt;{
  &quot;authors&quot;: [
    {
      &quot;author&quot;: &quot;Hieu Nguyen&quot;,
      &quot;authorURL&quot;: &quot;https://dmi.unibas.ch/de/personen/hieu-huu-nguyen/&quot;,
      &quot;affiliation&quot;: &quot;Department of Mathematics and Computer Science, University of Basel&quot;,
      &quot;affiliationURL&quot;: &quot;https://dmi.unibas.ch/en/&quot;
    },
    {
      &quot;author&quot;: &quot;Yen-Hsi Richard Tsai&quot;,
      &quot;authorURL&quot;: &quot;https://www.ma.utexas.edu/users/ytsai/&quot;,
      &quot;affiliation&quot;: &quot;Department of Mathematics and Oden Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    }
  ]
  }&lt;/script&gt;

&lt;/d-front-matter&gt;

&lt;d-title&gt;
 &lt;div style=&quot;grid-column: screen; text-align: center&quot;&gt;
  &lt;h1&gt;Numerical Wave Propagation Aided by Deep Learning&lt;/h1&gt;
    
    &lt;img style=&quot;display: block; max-width: 15%; margin-top: 3rem; margin-bottom: 3rem; margin-left: auto; margin-right: auto; border-radius: 5%&quot; alt=&quot;Numerical Wave Propagation Aided by Deep Learning&quot; src=&quot;https://tsairesearch.github.io//assets/images/wavelnn/testvelmodels.png&quot; /&gt;
    
 &lt;/div&gt;


&lt;/d-title&gt;

&lt;d-article&gt;

&lt;p&gt; We propose a deep learning approach for wave propagation in media with multiscale wave speed, using a second-order linear wave equation model. We use neural networks to enhance the accuracy of a given inaccurate coarse solver, which under-resolves a class of multiscale wave media and wave fields of interest. Our approach involves generating training data by the given computationally efficient coarse solver and another sufficiently accurate solver, applied to a class of wave media (described by their wave speed profiles) and initial wave fields. We find that the trained neural networks can approximate the nonlinear dependence of the propagation on the wave speed as long as the causality is appropriately sampled in training data. We combine the neural-network-enhanced coarse solver with the parareal algorithm and demonstrate that the coupled approach improves the stability of parareal algorithms for wave propagation and improves the accuracy of the enhanced coarse solvers. &lt;/p&gt;


&lt;h3&gt; Simulation &lt;/h3&gt;


&lt;div style=&quot;column-grid: middle &quot;&gt;
&lt;figure&gt;
&lt;div style=&quot;display: block; width: 90%; text-align: center&quot;&gt;
&lt;iframe width=&quot;881px&quot; height=&quot;703px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io//assets/images/wavelnn/testvelmodels.png&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;figcaption&gt;

&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;/div&gt;

&lt;h3&gt; Training databases &lt;/h3&gt;
&lt;p&gt;
&lt;a href=&quot;https://utexas.box.com/s/av7cnf3shgwf9m42snrzfh49pyt9x9y0&quot;&gt; Click Here&lt;/a&gt; to get the training databases. &lt;/p&gt;




&lt;h3&gt; Publications &lt;/h3&gt;

&lt;p&gt;
&lt;li&gt; Nguyen, H., &amp;amp; Tsai, R. (2023). Numerical wave propagation aided by deep learning. Journal of Computational Physics, 475, 111828. &lt;/li&gt;
&lt;/p&gt;

&lt;!-- H. Nguyen and R. Tsai. Numerical Wave Propagation Aided by Deep Learning. ArXiv:2107.13184, 2021--&gt;

&lt;/d-article&gt;</content><author><name></name></author><category term="projects" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/wavelnn/testvelmodels.png" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/wavelnn/testvelmodels.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deep learning and the Low Dimensional Manifold Hypothesis</title><link href="https://tsairesearch.github.io/projects/lowdim" rel="alternate" type="text/html" title="Deep learning and the Low Dimensional Manifold Hypothesis" /><published>2021-06-30T00:00:00-05:00</published><updated>2021-06-30T00:00:00-05:00</updated><id>https://tsairesearch.github.io/projects/lowdim</id><content type="html" xml:base="https://tsairesearch.github.io/projects/lowdim">&lt;d-front-matter&gt;

  &lt;script type=&quot;text/json&quot;&gt;{
  &quot;authors&quot;: [
    {
      &quot;author&quot;: &quot;Juncai He&quot;,
      &quot;authorURL&quot;: &quot;https://juncaihe.github.io/&quot;,
      &quot;affiliation&quot;: &quot;Department of Mathematics, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.ma.utexas.edu/&quot;
    },
    {
      &quot;author&quot;: &quot;Yen-Hsi Richard Tsai&quot;,
      &quot;authorURL&quot;: &quot;https://www.ma.utexas.edu/users/ytsai/&quot;,
      &quot;affiliation&quot;: &quot;Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.ices.utexas.edu&quot;
    },
    {
      &quot;author&quot;: &quot;Rachel Ward&quot;,
      &quot;authorURL&quot;: &quot;https://sites.google.com/prod/view/rward&quot;,
      &quot;affiliation&quot;: &quot;Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.ices.utexas.edu&quot;
    }
  ]
  }&lt;/script&gt;
&lt;/d-front-matter&gt;

&lt;d-title&gt;

&lt;div style=&quot;grid-column: screen; text-align: center&quot;&gt;
  &lt;h1&gt;Deep learning and the Low Dimensional Manifold Hypothesis&lt;/h1&gt;
    
    &lt;img style=&quot;display: block; max-width: 15%; margin-top: 3rem; margin-bottom: 3rem; margin-left: auto; margin-right: auto; border-radius: 5%&quot; alt=&quot;Deep learning and the Low Dimensional Manifold Hypothesis&quot; src=&quot;https://tsairesearch.github.io//assets/images/lowdim/streamplot_sigma.png&quot; /&gt;
    
 &lt;/div&gt;
&lt;/d-title&gt;

&lt;d-article&gt;

&lt;p&gt;
The low dimensional manifold hypothesis posits that the data found in many applications, such as those involving natural images, lie (approximately) on low dimensional manifolds embedded in a high dimensional Euclidean space. In this setting, a typical neural network defines a function that takes a finite number of vectors in the embedding space as input.
However, one often needs to consider evaluating the optimized network at points outside the training distribution. This project considers the case in which the training data is distributed in a linear subspace of Rd. We derive estimates on the variation of the learning function, defined by a neural network, in the direction transversal to the subspace. We study the potential regularization effects associated with the network’s depth and noise in the codimension of the data manifold. We also present additional side effects in training due to the presence of noise.
&lt;/p&gt;
&lt;h3&gt; Main results &lt;/h3&gt;

&lt;p&gt;
&lt;ol&gt;
  &lt;li&gt;If the data points, including noise, lie on $M$, the linear network’s depth may
provide certain regularization or side effects. For ReLU neural networks, we prove that $\frac{\partial f}{\partial n_M}$ is sensitive to the initialization of a set of “untrainable” parameters.&lt;/li&gt;
  &lt;li&gt;If the noise has a small positive variance in the orthogonal complement of $M$,
then:
&lt;ul&gt;
  &lt;li&gt;$\frac{\partial f}{\partial n_M}$ can be made arbitrarily small, provided that the number of data points scales according to some inverse power of the variance for deep linear neural networks and nonlinear deep linear neural networks;&lt;/li&gt;
  &lt;li&gt;For linear neural network models, it may take a gradient descent algorithm exponentially (in the reciprocal of the variance) long to converge to the unique optimal model parameters, which yield small $\frac{\partial f}{\partial n_M}$. In addition, it may also need a long time to escape the near region of origin.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
  &lt;li&gt;The stability-accuracy trade-off. The role of noise can be interpreted as a stabilizer for a model when evaluated on points outside of the (clean) data distribution. However, adding noise to the data set will impact of the accuracy of the network's generalization error (for evaluation within the data distribution). For nonlinear data manifolds, uniform noise may render the labeled data incompatible.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;
&lt;h3&gt; Publications &lt;/h3&gt;

&lt;p&gt;
&lt;li&gt; He, J., Tsai, R., &amp;amp; Ward, R. (2023). Side effects of learning from low-dimensional data embedded in a Euclidean space. Research in the Mathematical Sciences, 10(1), 13.
&lt;/li&gt;
&lt;/p&gt;
&lt;!--J. He, R. Tsai and R. Ward. &quot;Side-effects of Learning from Low Dimensional Data Embedded in an Euclidean Space&quot;. ArXiv: 2203.00614, 2022.
--&gt;

&lt;/d-article&gt;</content><author><name></name></author><category term="projects" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/lowdim/streamplot_sigma.png" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/lowdim/streamplot_sigma.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Selected Papers</title><link href="https://tsairesearch.github.io/papers" rel="alternate" type="text/html" title="Selected Papers" /><published>2019-11-09T00:00:00-06:00</published><updated>2019-11-09T00:00:00-06:00</updated><id>https://tsairesearch.github.io/papers</id><content type="html" xml:base="https://tsairesearch.github.io/papers">&lt;h3 id=&quot;exploration-and-surveillance&quot;&gt;Exploration and Surveillance&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Efficient and robust Sensor Placement in Complex Environments. Lukas Taus and Yen-Hsi Richard Tsai. Preprint 2023 &lt;a href=&quot;https://arxiv.org/abs/2309.08545&quot;&gt;[paper]&lt;/a&gt;&lt;a href=&quot;https://drive.google.com/file/d/12LGm468ZypvgAKlA1CVIV5Qu1ziiytb-/view&quot;&gt;[video]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Autonomous Exploration, Reconstruction, and Surveillance of 3d Environments aided by Deep Learning. Louis Ly and Yen-Hsi Richard Tsai. 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019.
&lt;a href=&quot;https://arxiv.org/abs/1809.06025&quot;&gt;[paper]&lt;/a&gt;
&lt;a href=&quot;http://helper.ipam.ucla.edu/publications/glws3/glws3_15672.pdf&quot;&gt;[slides]&lt;/a&gt;
&lt;a href=&quot;http://www.ipam.ucla.edu/abstract/?tid=15672&amp;amp;pcode=GLWS3&quot;&gt;[talk]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Strategy Synthesis for Surveillance-evasion Games with Learning-enabled Visibility Optimization. Suda Bharadwaj, Louis Ly, Bo Wu, Yen-Hsi Richard Tai, and Ufuk Topcu. 2019 Conference on Decisions and Control (CDC). IEEE, 2019.
&lt;a href=&quot;https://arxiv.org/abs/1911.07394&quot;&gt;[paper]&lt;/a&gt;
&lt;a href=&quot;http://helper.ipam.ucla.edu/publications/glws3/glws3_15672.pdf&quot;&gt;[slides]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An Efficient Algorithm for a Visibility-based Surveillance-evasion Game. Ryo Takei, Yen-Hsi Richard Tsai, Zhengyuan Zhou, and Yanina Landa. Communications in Mathematical Sciences 12.7 (2014): 1303-1327.
&lt;a href=&quot;https://www.intlpress.com/site/pub/pages/journals/items/cms/content/vols/0012/0007/a007/&quot;&gt;[paper]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A Practical Path-planning Algorithm for a Simple Car: a Hamilton-Jacobi Approach. Ryo Takei, Yen-Hsi Richard Tsai, Haochong Shen, and Yanina Landa. Proceedings of the 2010 American control conference. IEEE, 2010.
&lt;a href=&quot;ftp://ftp.math.ucla.edu/pub/camreport/cam09-74.pdf&quot;&gt;[paper]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;point-cloud-processing&quot;&gt;Point Cloud Processing&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Nearest Neighbor Sampling of Point Sets using Rays. Liangchen Liu, Louis Ly, Colin Macdonald, and Yen-Hsi Richard Tsai. Communications on Applied Mathematics and Computation 2023,
&lt;a href=&quot;https://arxiv.org/abs/1911.10737&quot;&gt;[paper]&lt;/a&gt;&lt;a href=&quot;https://drive.google.com/file/d/1U3imtMJiJrSDqYZ5knMYNCKo9vTbWbdZ/view?usp=share_link&quot;&gt;[poster]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;manifold-structured-data-and-its-impact-on-learning&quot;&gt;Manifold Structured Data and its Impact on Learning&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Linear Regression on Manifold Structured Data: the Impact of Extrinsic Geometry on Solutions. Liangchen Liu, Juncai He, and Richard Tsai. Topology, Algebra and Geometry in Machine Learning workshop, ICML2023 &lt;a href=&quot;https://arxiv.org/abs/2307.02478&quot;&gt;[paper]&lt;/a&gt;&lt;a href=&quot;https://icml.cc/media/PosterPDFs/ICML%202023/27589.png?t=1691533674.7285285&quot;&gt;[poster]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Side-effects of Learning from Low Dimensional Data Embedded in an Euclidean Space. Juncai He, Richard Tsai and Rachel Ward. Research in Mathematical Science 10.13 (2023).  &lt;a href=&quot;https://doi.org/10.1007/s40687-023-00378-y&quot;&gt;[paper]&lt;/a&gt;]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;source-localization&quot;&gt;Source Localization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Autonomous Source Discovery and Navigation in Complicated Environments. Yanina Landa,  Haochong Shen, Ryo Takei, Yen-Hsi Richard Tsai. Preprint 2009.
&lt;a href=&quot;ftp://ftp.math.ucla.edu/pub/camreport/cam09-73.pdf&quot;&gt;[paper]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Discovery of Point Sources in the Helmholtz Equation posed in Unknown Domains with Obstacles. Yanina Landa, Nicolay Tanushev, Yen-Hsi Richard Tsai. Communications in Mathematical Sciences 9.3 (2011): 903-928.
&lt;a href=&quot;https://www.intlpress.com/site/pub/pages/journals/items/cms/content/vols/0009/0003/a011/&quot;&gt;[paper]&lt;/a&gt;
&lt;a href=&quot;https://www.dropbox.com/s/h3pesvk99rom90r/inverse_source_problem_nonlinear-helmholtz.pdf?dl=0&quot;&gt;[slides]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Point Source Identification in Nonlinear Advection–Diffusion–Reaction Systems. Alexander Mamonov and Yen-Hsi Richard Tsai. Inverse Problems 29.3 (2013): 035009.
&lt;a href=&quot;ftp://ftp.math.ucla.edu/pub/camreport/cam12-15.pdf&quot;&gt;[paper]&lt;/a&gt;
&lt;a href=&quot;https://www.dropbox.com/s/mnk5a3j3xiozh43/srcid-talk-richard.pdf?dl=0&quot;&gt;[slides]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heat Source Identification based on Constrained Minimization. Yingying Li, Stanley Osher, and Yen-Hsi Richard Tsai. Inverse Problems and Imaging 8.1 (2014): 199-221.
&lt;a href=&quot;https://www.researchgate.net/profile/Richard_Tsai3/publication/264998169_Heat_source_identification_based_on_L1_constrained_minimization/links/54cbafab0cf24601c088b3d8/Heat-source-identification-based-on-L1-constrained-minimization.pdf&quot;&gt;[paper]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;multiscale-coupling-algorithms&quot;&gt;Multiscale Coupling Algorithms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Stabilization of parareal algorithms for long time computation of a class of highly oscillatory Hamiltonian flows using data. Preprint (2023) &lt;a href=&quot;https://arxiv.org/pdf/2309.01225.pdf&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;A Stable Parareal-like Method for the Second Order Wave Equation. Hieu Nguyen and Richard Tsai. Journal of Computational Physics (2020). Accepted.
&lt;a href=&quot;https://doi.org/10.1016/j.jcp.2019.109156&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
### Deep Neural Networks and Finite Element Methods

- ReLU deep neural networks from the hierarchical basis perspective. Juncai He, Lin Li and Jinchao Xu. Preprint 2021. [[paper]](https://arxiv.org/abs/2105.04156)
- ReLU deep neural networks and linear finite elements. Juncai He, Lin Li, Jinchao Xu and Chunyue Zheng. Journal of Computational Mathematics 38.3 (2020): 502-527. [[paper]](https://www.global-sci.org/intro/article_detail/jcm/15798.html)
- Power series expansion neural network. Qipin Chen, Wenrui Hao and Juncai He. Preprint 2021. [[paper]](https://arxiv.org/abs/2102.13221)

### Convolutional neural networks and multigrid

- Constrained Linear Data-feature Mapping in Image Classification. Juncai He, Yuyan Chen, Lian Zhang and Jinchao Xu. Preprint 2020. [[paper]](https://arxiv.org/abs/1911.10428)
- MgNet: a unified framework of multigrid and convolutional neural network. Juncai He and Jinchao Xu.  Science China Mathematics 62.7 (2019): 1331–1354. [[paper]](https://link.springer.com/article/10.1007%2Fs11425-019-9547-2)

### Training algorithms in deep learning

-  Make ![ell_1](https://juncaihe.github.io/eqs/2349274112114421517-130.png) regularization effective in training sparse CNN. Juncai He, Xiaodong Jia, Jinchao Xu, Liang Zhao and Lian Zhang. Computational Optimization and Applications (2020). [[paper]](https://link.springer.com/article/10.1007/s10589-020-00202-1) --&gt;</content><author><name></name></author><category term="papers" /><summary type="html">Exploration and Surveillance Efficient and robust Sensor Placement in Complex Environments. Lukas Taus and Yen-Hsi Richard Tsai. Preprint 2023 [paper][video]</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/github_avatar.jpg" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/github_avatar.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Mathematics in Deep Learning —Syllabus</title><link href="https://tsairesearch.github.io/notes/syllabus" rel="alternate" type="text/html" title="Mathematics in Deep Learning —Syllabus" /><published>2019-10-20T00:00:00-05:00</published><updated>2019-10-20T00:00:00-05:00</updated><id>https://tsairesearch.github.io/notes/syllabus</id><content type="html" xml:base="https://tsairesearch.github.io/notes/syllabus">&lt;p&gt;Welcome to the zoo!&lt;/p&gt;

&lt;p&gt;A tentative list of topics to be covered in this course.&lt;/p&gt;

&lt;p&gt;The course will be conducted with a mixture of regular lectures, seminar style presentation and discussion.&lt;/p&gt;

&lt;p&gt;Participants of this course are expected to present certain relevant concepts from suggested reading assignments, and arrange the presentation in a certain uniform style.&lt;/p&gt;

&lt;h3 id=&quot;deep-learning-architectures-and-the-related-applications&quot;&gt;Deep learning architectures and the related applications:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;basic multi-layer neural networks (NNs)&lt;/li&gt;
  &lt;li&gt;convolutional Neural networks (CNNs)&lt;/li&gt;
  &lt;li&gt;residual neural networks (ResNets)&lt;/li&gt;
  &lt;li&gt;generative adversarial networks (GANs) and the Wasserstein GANs&lt;/li&gt;
  &lt;li&gt;recurrent neural network (RNN)&lt;/li&gt;
  &lt;li&gt;LSTMs&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;convolutional Neural networks for graphs&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;encoder-decoders&lt;/li&gt;
  &lt;li&gt;reinforcement learning (value iteration, policy iteration)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;algorithmic-components&quot;&gt;Algorithmic components:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;stochastic gradient descent algorithms&lt;/li&gt;
  &lt;li&gt;backpropagation and automatic differentiation&lt;/li&gt;
  &lt;li&gt;computational graphs&lt;/li&gt;
  &lt;li&gt;search algorithms: Monte-Carlo Tree Search used in AlphaGo&lt;/li&gt;
  &lt;li&gt;classical multi-level algorithms: multigrid methods&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;approximation-theory&quot;&gt;Approximation theory:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;classical multi-resolution analysis (wavelet)&lt;/li&gt;
  &lt;li&gt;compressive sensing&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The curse of dimensionality!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;single layer universal approximation theory&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;multi-layer neural network approximation theory&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;over-parameterization and generalization&lt;/li&gt;
  &lt;li&gt;linear algebra: defining notions of distances from data matrices&lt;/li&gt;
  &lt;li&gt;manifold learning&lt;/li&gt;
  &lt;li&gt;transfer learning&lt;/li&gt;
  &lt;li&gt;adversarial attacks&lt;/li&gt;
  &lt;li&gt;differential privacy&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;random-graphs-and-random-matrices&quot;&gt;Random graphs and random matrices:&lt;/h3&gt;

&lt;h3 id=&quot;optimization-algorithms-and-theories&quot;&gt;Optimization algorithms and theories:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;efficient algorithms for finding a/the minimum of a loss function&lt;/li&gt;
  &lt;li&gt;mini-batch and minimizing variances&lt;/li&gt;
  &lt;li&gt;duality, saddle point problems&lt;/li&gt;
  &lt;li&gt;primal-dual type splitting algorithms&lt;/li&gt;
  &lt;li&gt;Nesterov’s algorithm&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;how to select your mini-batches?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;which loss function?&lt;/li&gt;
  &lt;li&gt;the vanishing gradient problem of using &lt;em&gt;Sigmoids&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;cross-entropy&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;training-issues&quot;&gt;Training issues&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The problem with Sigmoids: Sigmoid “saturates” by approaching 1, as the input increases. (ReLU just keeps increasing)  When Sigmoid(x) is very close to 1, it’s gradient is very close to 0, and gives little information for gradient descent algorithms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The dying ReLu problem: ReLU neurons becomes inactive (only outputs zero regardless of inputs. Little theoretical results about this phen
 omenon).
    &lt;ul&gt;
      &lt;li&gt;Remedy type 1: modify the network architecture, and possibly replace the activation function&lt;/li&gt;
      &lt;li&gt;Remedy type 2: introduce additional training steps – normalization techniques and introducing “dropouts”&lt;/li&gt;
      &lt;li&gt;Remedy type 3: initialization&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;batch normalization (Cf. Ioffe and Szegedy, 2015): It is a technique that inserts layers into the deep neural network that transform the output for the batch to be zero mean unit variance.&lt;/li&gt;
  &lt;li&gt;weight initialization: carefully choose random initial weights with suitable variances (Cf: Hanin and Rolnick, How to start training: the effect of initialization and architecture)&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;initialization-for-training-neural-networks&quot;&gt;Initialization for training neural networks&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;For ReLU networks: Karniadakis proposes randomize asymmetric initialization&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dynamical-system&quot;&gt;Dynamical system:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;stability&lt;/li&gt;
  &lt;li&gt;automatic step size control&lt;/li&gt;
  &lt;li&gt;optimal control of dynamical systems&lt;/li&gt;
  &lt;li&gt;Decoupling algorithms&lt;/li&gt;
  &lt;li&gt;Impulse method&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimal-transport-theory-and-algorithms&quot;&gt;Optimal transport theory and algorithms:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Comparing probability densities&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Earth mover’s distances “convexify”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The Benamou-Brenier fluid formulation and related algorithms&lt;/li&gt;
  &lt;li&gt;Sinkhorn algorithm&lt;/li&gt;
  &lt;li&gt;Information Geometry&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mean-field-games&quot;&gt;Mean field games&lt;/h3&gt;

&lt;h3 id=&quot;some-novel-applications&quot;&gt;Some novel applications:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Beyond image processing and facial recognition&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;deep-learning-for-scientific-computing&quot;&gt;Deep learning for scientific computing:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;wave propagation&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Louis and Richard</name></author><category term="notes" /><summary type="html">Welcome to the zoo!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/github_avatar.jpg" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/github_avatar.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Multiscale coupling algorithms using parareal style structures</title><link href="https://tsairesearch.github.io/projects/parareal" rel="alternate" type="text/html" title="Multiscale coupling algorithms using parareal style structures" /><published>2019-10-04T00:00:00-05:00</published><updated>2019-10-04T00:00:00-05:00</updated><id>https://tsairesearch.github.io/projects/parareal</id><content type="html" xml:base="https://tsairesearch.github.io/projects/parareal">&lt;d-front-matter&gt;
  &lt;script type=&quot;text/json&quot;&gt;{
  &quot;authors&quot;: [
    {
      &quot;author&quot;: &quot;Hieu Nguyen&quot;,
      &quot;authorURL&quot;: &quot;https://www.oden.utexas.edu/&quot;,
      &quot;affiliation&quot;: &quot;Oden Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    },
    {
      &quot;author&quot;: &quot;Yen-Hsi Richard Tsai&quot;,
      &quot;authorURL&quot;: &quot;https://www.ma.utexas.edu/users/ytsai/&quot;,
      &quot;affiliation&quot;: &quot;Oden Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    }
  ]
  }&lt;/script&gt;
&lt;/d-front-matter&gt;

&lt;d-title&gt;
 &lt;div style=&quot;grid-column: screen; text-align: center&quot;&gt;
  &lt;h1&gt;Multiscale coupling algorithms using parareal style structures&lt;/h1&gt;
    
    &lt;img style=&quot;display: block; max-width: 15%; margin-top: 3rem; margin-bottom: 3rem; margin-left: auto; margin-right: auto; border-radius: 5%&quot; alt=&quot;Multiscale coupling algorithms using parareal style structures&quot; src=&quot;https://tsairesearch.github.io//assets/images/wave/waveinmarmousi.gif&quot; /&gt;
    
 &lt;/div&gt;


&lt;/d-title&gt;

&lt;d-article&gt;

&lt;p&gt;We are developing a data-driven parallel-in-time iterative method to solve the homogeneous second-order wave equation. The new method involves a coarse-scale propagator and a fine-scale propagator which fully resolves the medium using finer spatial grid and shorter time steps. The fine-scale propagator is run in parallel for short time subintervals. The two propagators are coupled in an iterative way similar to the standard parareal method. We train a Neural Network, which structure mimics the physics of wave propagation, to enhance the accuracy of the coarse-scale propagator such that the parareal iteration stabilizes and hence converges.&lt;/p&gt;

&lt;h3&gt; Simulation &lt;/h3&gt;


&lt;div style=&quot;column-grid: middle &quot;&gt;

&lt;figure&gt;
&lt;div style=&quot;display: block; width: 90%; text-align: center&quot;&gt;
&lt;iframe width=&quot;547px&quot; height=&quot;547px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io/assets/images/wave/waveinmarmousi.gif&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;figcaption&gt;
Wave propagating through Marmousi velocity profile.
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;/div&gt;


&lt;h3&gt; Publications &lt;/h3&gt;

&lt;p&gt;
&lt;li&gt; H. Nguyen, and R. Tsai. &quot;A stable parareal-like method for the second order wave equation.&quot; Journal of Computational Physics (2020). Accepted.&lt;/li&gt;
&lt;li&gt; G. Ariel, H. Nguyen, and R. Tsai. &quot;\theta-parareal schemes.&quot; arXiv preprint arXiv:1704.06882 (2017). &lt;/li&gt;
&lt;/p&gt;



&lt;/d-article&gt;</content><author><name></name></author><category term="projects" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/wave/waveinmarmousi.gif" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/wave/waveinmarmousi.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>