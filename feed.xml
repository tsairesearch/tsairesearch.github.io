<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="https://tsairesearch.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tsairesearch.github.io/" rel="alternate" type="text/html" /><updated>2023-11-22T14:52:08-06:00</updated><id>https://tsairesearch.github.io/feed.xml</id><title type="html">The Scientific AI Research Group</title><entry><title type="html">Sampling and Processing of Point Clouds</title><link href="https://tsairesearch.github.io/projects/rays" rel="alternate" type="text/html" title="Sampling and Processing of Point Clouds" /><published>2023-09-13T00:00:00-05:00</published><updated>2023-09-13T00:00:00-05:00</updated><id>https://tsairesearch.github.io/projects/rays</id><content type="html" xml:base="https://tsairesearch.github.io/projects/rays">&lt;!-- define the style for images container --&gt;
&lt;head&gt;
&lt;style&gt;
  div.container {
     display:inline-block;
   }

  html,body        {height: 100%;}

  .wrapper{width: 80%; max-width: 600px; height: 100%; margin: 0 auto; background: #CCC}

  .h_iframe{position: relative; padding-top: 56%;}

  .h_iframe iframe{position: absolute; top: 0; left: 0; width: 100%; height: 100%;}
&lt;/style&gt;
&lt;/head&gt;

&lt;body&gt;
&lt;d-front-matter&gt;
  &lt;script type=&quot;text/json&quot;&gt;{
  &quot;authors&quot;: [
    {
      &quot;author&quot;: &quot;Liangchen (Lewis) Liu&quot;,
      &quot;authorURL&quot;: &quot;https://lclewis.github.io&quot;,
      &quot;affiliation&quot;: &quot;Department of Mathematics, The University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;http://ma.utexas.edu/&quot;
    },
    {
      &quot;author&quot;: &quot;Louis Ly&quot;,
      &quot;authorURL&quot;: &quot;https://longlouisly.github.io/&quot;,
      &quot;affiliation&quot;: &quot;Oden Institute for Computational Engineering and Sciences, The University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.ices.utexas.edu&quot;
    },
    {
      &quot;author&quot;: &quot;Colin Macdonald&quot;,
      &quot;authorURL&quot;: &quot;https://www.math.ubc.ca/~cbm/&quot;,
      &quot;affiliation&quot;: &quot;Department of Mathematics, The University of British Columbia&quot;,
      &quot;affiliationURL&quot;: &quot;https://ubc.ca&quot;
    },

    {
      &quot;author&quot;: &quot;Yen-Hsi Richard Tsai&quot;,
      &quot;authorURL&quot;: &quot;https://www.ma.utexas.edu/users/ytsai/&quot;,
      &quot;affiliation&quot;: &quot;Oden Institute for Computational Engineering and Sciences, The University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    }
  ]
  }&lt;/script&gt;
&lt;/d-front-matter&gt;


&lt;d-title&gt;
 &lt;div style=&quot;grid-column: screen; text-align: center&quot;&gt;
  &lt;h1&gt;Sampling and Processing of Point Clouds&lt;/h1&gt;
    
    &lt;img style=&quot;display: block; max-width: 15%; margin-top: 3rem; margin-bottom: 3rem; margin-left: auto; margin-right: auto; border-radius: 5%&quot; alt=&quot;Sampling and Processing of Point Clouds&quot; src=&quot;https://tsairesearch.github.io//assets/images/rays/airplane_ray.png&quot; /&gt;
    
 &lt;/div&gt;


&lt;/d-title&gt;


&lt;d-article&gt;

&lt;p&gt; We propose a new framework for the sampling, compression, and analysis of distributions of point sets and other geometric objects embedded in Euclidean spaces. Our approach involves constructing a tensor called the RaySense sketch, which captures nearest neighbors from the underlying geometry of points along a set of rays. We explore various operations that can be performed on the RaySense sketch, leading to different properties and potential applications. Statistical information about the data set can be extracted from the sketch, independent of the ray set. Line integrals on point sets can be efficiently computed using the sketch. We also present several examples illustrating applications of the proposed strategy in practical scenarios. &lt;/p&gt;

&lt;h2&gt;  Visualizations of Method &lt;/h2&gt;

&lt;h3&gt; Sampling Visualization with rays and Features &lt;/h3&gt;
&lt;div style=&quot;grid-column: screen &quot;&gt;

&lt;!-- &lt;figure&gt;
&lt;div style=&quot;display: block; width: 80%; text-align: center&quot;&gt;
&lt;iframe width=&quot;1296px&quot; height=&quot;850px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io/assets/images/rays/grid.png&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt; --&gt;


&lt;figure&gt;
&lt;div style=&quot;display: block; width: 80%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
&lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/grid.png&quot; width=&quot;338px&quot; height=&quot;648px&quot; /&gt;
&lt;/div&gt;

&lt;!-- &lt;div class=&quot;wrapper&quot;&gt;
    &lt;div class=&quot;h_iframe&quot;&gt;
      &lt;iframe height=&quot;4&quot; width=&quot;8&quot; src=&quot;https://tsairesearch.github.io/assets/images/rays/grid.png&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;
    &lt;/div&gt;
&lt;/div&gt; --&gt;
&lt;!-- &lt;div class=&quot;container&quot;&gt;
    &lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/grid.png&quot; height=&quot;1024&quot; width=&quot;768&quot;/&gt;
&lt;/div&gt; --&gt;

&lt;figcaption&gt;
&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
(Row 1) Visualization of a two rays (black) through points sampled from various objects (gray). Closest point pairs are shown in green and red. (Row 2) Plot of distance from points along the ray to the corresponding closest points on the object. (Rows 3-5) The $x$, $y$, and $z$ coordinates of the closest points to the ray.
&lt;/div&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;

&lt;h3&gt; Visualization of Voronoi Cell with Rays &lt;/h3&gt;
&lt;div style=&quot;grid-column: screen &quot;&gt;

&lt;figure&gt;
&lt;div style=&quot;display: block; width: 100%; text-align: center&quot;&gt;
&lt;iframe width=&quot;800px&quot; height=&quot;365px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io/assets/images/rays/voronoi.png&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;figcaption&gt;
&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
Two rays (black)
sense nearest neighbors of the point set (blue). Singular
points, such as the tip of the tail, have larger Voronoi cells
(dashed lines) and are more likely to be sampled. Closest
point pairs are shown in green and red.

&lt;/div&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;

&lt;h3&gt; Visualization of Sampling Frequencies&lt;/h3&gt;
&lt;div style=&quot;grid-column: screen &quot;&gt;

&lt;figure&gt;
&lt;div style=&quot;display: block; width: 50%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
&lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/airplane_side.png&quot; width=&quot;210px&quot; height=&quot;350px&quot; /&gt;
&lt;/div&gt;

&lt;figcaption&gt;
&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
Rays are more likely to sample salient features in the point cloud. Larger points are repeated more often.
We can control the number of points by increasing the number of rays. Each ray contains 30 sample points.  
&lt;/div&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;



&lt;h2&gt; Downstream Applications &lt;/h2&gt;


  &lt;h3&gt; Salient Points Detection &lt;/h3&gt;

    &lt;figure class=&quot;half&quot; style=&quot;display:flex&quot;&gt;

      &lt;div class=&quot;container&quot;&gt;

        &lt;div style=&quot;display: block; width: 90%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
        &lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/MNIST_Salient_RaySense_top10_01cube-202210-noFreq.png&quot; width=&quot;210px&quot; height=&quot;350px&quot; /&gt;
        &lt;/div&gt;


        &lt;figcaption&gt;
        &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
          RaySense Sampling with Top 10 Frequencies

        &lt;/div&gt;
        &lt;/figcaption&gt;
      &lt;/div&gt;

      &lt;div class=&quot;container&quot;&gt;
        &lt;div style=&quot;display: block; width: 90%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
        &lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/MNIST_random_10-202210-noFreq.png&quot; width=&quot;210px&quot; height=&quot;350px&quot; /&gt;
        &lt;/div&gt;

        &lt;figcaption&gt;
        &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
          Uniformly Random Sampling
          &lt;/div&gt;
        &lt;/figcaption&gt;
    &lt;/div&gt;
    &lt;/figure&gt;

        &lt;figure&gt;
          &lt;figcaption&gt;
          &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 100%; text-align: center&quot;&gt;
          Comparison of RaySense sampled points and points from uniformly random sampling of the MNIST dataset. RaySense sampled points are those with the highest RaySense sampling frequencies for each class. These points correspond to digits written using unusual strokes, hence giving a notion of salient points of the dataset.
          &lt;/div&gt;
          &lt;/figcaption&gt;
        &lt;/figure&gt;



    &lt;h3&gt; Radon Transform and Reconstruction&lt;/h3&gt;

        &lt;figure class=&quot;half&quot; style=&quot;display:flex&quot;&gt;

          &lt;div class=&quot;container&quot;&gt;

            &lt;div style=&quot;display: block; width: 90%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
            &lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/point_cloud_and_density.png&quot; width=&quot;210px&quot; height=&quot;160px&quot; /&gt;
            &lt;/div&gt;


            &lt;figcaption&gt;
            &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
              Underlying density and point sets
            &lt;/div&gt;
            &lt;/figcaption&gt;
          &lt;/div&gt;

          &lt;div class=&quot;container&quot;&gt;
            &lt;div style=&quot;display: block; width: 90%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
            &lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/example_M21.png&quot; width=&quot;210px&quot; height=&quot;160px&quot; /&gt;
            &lt;/div&gt;

            &lt;figcaption&gt;
            &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
              RaySensed points from uniform lines
              &lt;/div&gt;
            &lt;/figcaption&gt;
        &lt;/div&gt;

        &lt;div class=&quot;container&quot;&gt;
          &lt;div style=&quot;display: block; width: 90%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
          &lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/sonogram_92441_M101_K64_dt1.png&quot; width=&quot;210px&quot; height=&quot;160px&quot; /&gt;
          &lt;/div&gt;

          &lt;figcaption&gt;
          &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
            Integral transformed / spectrum
            &lt;/div&gt;
          &lt;/figcaption&gt;
        &lt;/div&gt;

        &lt;div class=&quot;container&quot;&gt;
          &lt;div style=&quot;display: block; width: 90%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
          &lt;img src=&quot;https://tsairesearch.github.io/assets/images/rays/fbp_92441_M101_K64_dt1_trim.png&quot; width=&quot;210px&quot; height=&quot;160px&quot; /&gt;
          &lt;/div&gt;

          &lt;figcaption&gt;
          &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
            Reconstruction results
            &lt;/div&gt;
          &lt;/figcaption&gt;
        &lt;/div&gt;
        &lt;/figure&gt;

            &lt;figure&gt;
              &lt;figcaption&gt;
              &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 100%; text-align: center&quot;&gt;
            Approximate Radon transform computed with RaySense from point cloud data (a)–(c) and filtered back projection reconstruction (d). Our theoretical analysis also suggests the line integral error from RaySense approximation of an $N$-size point cloud in $\mathbb R^d$ converge in a rate of $\mathcal O\left(N^{-\frac{1}d}\right)$.
              &lt;/div&gt;
              &lt;/figcaption&gt;
            &lt;/figure&gt;


&lt;h3&gt; Classification using Deep Neural Networks &lt;/h3&gt;

&lt;div style=&quot;grid-column: screen &quot;&gt;
&lt;figure&gt;
&lt;div style=&quot;display: block; width: 100%; text-align: center&quot;&gt;
&lt;iframe width=&quot;900px&quot; height=&quot;250px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io/assets/images/rays/architecture.png&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;figcaption&gt;
&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
Neural network architecture for point cloud classification based on ray signatures.

&lt;/div&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
&lt;div style=&quot;display: block; width: 100%; text-align: center&quot;&gt;
&lt;iframe width=&quot;600px&quot; height=&quot;320px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io/assets/images/rays/missing_data.png&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;figcaption&gt;
&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
Comparison of our neural network (RayNN) against other prominent methods on ModelNet40 dataset with missing data.
Our model is robust, even when the data consists of only a few number of points.

&lt;/div&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;



&lt;/div&gt;




&lt;div style=&quot;grid-column: screen; margin-left: auto; margin-right: auto; text-align: center &quot;&gt;

&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 80%; text-align: center&quot;&gt;
&lt;figure&gt;
&lt;figcaption&gt;
&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 100%; text-align: center&quot;&gt;
Comparison of our deep neural network model against PointNet on ModelNet point cloud classification dataset.
&lt;/div&gt;
&lt;/figcaption&gt;


&lt;table&gt;
  &lt;tr&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;Model&lt;/th&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;ModelNet10 (2048)&lt;/th&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;ModelNet40 (2048)&lt;/th&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;ModelNet40 (1024)&lt;/th&gt;
    &lt;th class=&quot;tg-0pky&quot;&gt;ModelNet40 (4096)&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;PointNet (paper)&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;-&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;-&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;89.2%&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;-&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;PointNet.pytorch&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;92.07% (on 256 pts)&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;85.3% (on 1024 pts)&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;87.52%&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;85.4%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;Ours&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;95.04%&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;90.03%&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;90.6%&lt;/td&gt;
    &lt;td class=&quot;tg-0pky&quot;&gt;90.44%&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;/figure&gt;
&lt;/div&gt;

&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
&lt;figure&gt;
&lt;figcaption&gt;
&lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 100%; text-align: center&quot;&gt;
Sensitivity of the neural network models to the input size. Here, $m$ is the number of rays, $N^\ast$ is the number of points used, and $N$ is the total number of points in the original point set.
&lt;/div&gt;
&lt;/figcaption&gt;


&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;Our model on ModelNet10 (N=2048)&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;$m/32$&lt;/td&gt;
    &lt;td&gt;100%&lt;/td&gt;
    &lt;td&gt;50%&lt;/td&gt;
    &lt;td&gt;25%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;$N^\ast/N$&lt;/td&gt;
    &lt;td&gt;15.38%&lt;/td&gt;
    &lt;td&gt;8.59%&lt;/td&gt;
    &lt;td&gt;4.59%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Performance/Accuracy&lt;/td&gt;
    &lt;td&gt;94.60%&lt;/td&gt;
    &lt;td&gt;95.04%&lt;/td&gt;
    &lt;td&gt;94.60%&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;Our model on ModelNet40 (N=1024)&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;$m/32$&lt;/td&gt;
    &lt;td&gt;100%&lt;/td&gt;
    &lt;td&gt;50%&lt;/td&gt;
    &lt;td&gt;25%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;$N^\ast/N$&lt;/td&gt;
    &lt;td&gt;25.10%&lt;/td&gt;
    &lt;td&gt;14.75%&lt;/td&gt;
    &lt;td&gt;8.17%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Performance/Accuracy&lt;/td&gt;
    &lt;td&gt;90.56%&lt;/td&gt;
    &lt;td&gt;90.60%&lt;/td&gt;
    &lt;td&gt;89.82%&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;PointNet on ModelNet40 (N=1024)&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;$N^\ast/N$&lt;/td&gt;
    &lt;td&gt;100%&lt;/td&gt;
    &lt;td&gt;50%&lt;/td&gt;
    &lt;td&gt;12.5%&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Performance/Accuracy&lt;/td&gt;
    &lt;td&gt;89.2%&lt;/td&gt;
    &lt;td&gt;86.8%&lt;/td&gt;
    &lt;td&gt;69%&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;/figure&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;h2&gt; Publications &lt;/h2&gt;

&lt;p&gt;
&lt;li&gt;  
Liu, L., Ly, L., Macdonald, C. &amp;amp; Tsai, R. (2023) Nearest Neighbor Sampling of Point Sets using Rays. Communication on Applied Mathematics and Computation (CAMC), Focused Issue in Honor of Prof. Stanley Osher on the Occasion of His 80th Birthday. Accepted.
&lt;/li&gt;

&lt;/p&gt;

&lt;/d-article&gt;
&lt;/body&gt;</content><author><name></name></author><category term="projects" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/rays/airplane_ray.png" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/rays/airplane_ray.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Efficient and Robust Sensor Placement in Complex Environment</title><link href="https://tsairesearch.github.io/projects/sensor" rel="alternate" type="text/html" title="Efficient and Robust Sensor Placement in Complex Environment" /><published>2023-09-01T00:00:00-05:00</published><updated>2023-09-01T00:00:00-05:00</updated><id>https://tsairesearch.github.io/projects/sensor</id><content type="html" xml:base="https://tsairesearch.github.io/projects/sensor">&lt;d-front-matter&gt;
  &lt;script type=&quot;text/json&quot;&gt;{
  &quot;authors&quot;: [
    {
      &quot;author&quot;: &quot;Lukas Taus&quot;,
      &quot;authorURL&quot;: &quot;https://oden.utexas.edu/people/directory/Lukas-Taus&quot;,
      &quot;affiliation&quot;: &quot;Oden Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    },
    {
      &quot;author&quot;: &quot;Yen-Hsi Richard Tsai&quot;,
      &quot;authorURL&quot;: &quot;https://www.ma.utexas.edu/users/ytsai/&quot;,
      &quot;affiliation&quot;: &quot;Oden Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    }
  ]
  }&lt;/script&gt;
&lt;/d-front-matter&gt;

&lt;d-title&gt;
 &lt;div style=&quot;grid-column: screen; text-align: center&quot;&gt;
  &lt;h1&gt;Efficient and Robust Sensor Placement in Complex Environment&lt;/h1&gt;
    
    &lt;img style=&quot;display: block; max-width: 15%; margin-top: 3rem; margin-bottom: 3rem; margin-left: auto; margin-right: auto; border-radius: 5%&quot; alt=&quot;Efficient and Robust Sensor Placement in Complex Environment&quot; src=&quot;https://tsairesearch.github.io//assets/images/github_avatar.jpg&quot; /&gt;
    
 &lt;/div&gt;


&lt;/d-title&gt;

&lt;d-article&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h3&gt; Simulation &lt;/h3&gt;


&lt;div style=&quot;column-grid: middle &quot;&gt;

&lt;figure&gt;
&lt;div style=&quot;text-align: center&quot;&gt;
&lt;iframe src=&quot;https://drive.google.com/file/d/12LGm468ZypvgAKlA1CVIV5Qu1ziiytb-/preview&quot; width=&quot;640&quot; height=&quot;480&quot; allow=&quot;autoplay&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 &lt;figcaption&gt;
   [missing]
 &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;


&lt;h3&gt; Publications &lt;/h3&gt;

&lt;p&gt;
&lt;li&gt;  
Taus, L., &amp;amp; Tsai, R. (2023) Efficient and robust Sensor Placement in Complex Environments. Preprint.
&lt;/li&gt;

&lt;/p&gt;



&lt;/d-article&gt;</content><author><name></name></author><category term="projects" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/github_avatar.jpg" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/github_avatar.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Impact of the Extrinsic Geometry on Linear Regression</title><link href="https://tsairesearch.github.io/projects/extrinsic_geom" rel="alternate" type="text/html" title="Impact of the Extrinsic Geometry on Linear Regression" /><published>2023-06-30T00:00:00-05:00</published><updated>2023-06-30T00:00:00-05:00</updated><id>https://tsairesearch.github.io/projects/extrinsic_geom</id><content type="html" xml:base="https://tsairesearch.github.io/projects/extrinsic_geom">&lt;head&gt;
&lt;style&gt;
  div.container {
     display:inline-block;
   }

  html,body        {height: 100%;}

  .wrapper{width: 80%; max-width: 600px; height: 100%; margin: 0 auto; background: #CCC}

  .h_iframe{position: relative; padding-top: 56%;}

  .h_iframe iframe{position: absolute; top: 0; left: 0; width: 100%; height: 100%;}
&lt;/style&gt;
&lt;/head&gt;

&lt;d-front-matter&gt;

  &lt;script type=&quot;text/json&quot;&gt;{
  &quot;authors&quot;: [
    {
      &quot;author&quot;: &quot;Liangchen (Lewis) Liu&quot;,
      &quot;authorURL&quot;: &quot;https://lclewis.github.io/&quot;,
      &quot;affiliation&quot;: &quot;Department of Mathematics, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.ma.utexas.edu/&quot;
    },
    {
      &quot;author&quot;: &quot;Juncai He&quot;,
      &quot;authorURL&quot;: &quot;https://juncaihe.github.io/&quot;,
      &quot;affiliation&quot;: &quot;Applied Mathematics and Computational Sciences, The King Abdullah University of Science and Technology (KAUST)&quot;,
      &quot;affiliationURL&quot;: &quot;https://cemse.kaust.edu.sa/amcs&quot;
    },
    {
      &quot;author&quot;: &quot;Yen-Hsi Richard Tsai&quot;,
      &quot;authorURL&quot;: &quot;https://www.ma.utexas.edu/users/ytsai/&quot;,
      &quot;affiliation&quot;: &quot;Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.ices.utexas.edu&quot;
    }

  ]
  }&lt;/script&gt;
&lt;/d-front-matter&gt;

&lt;d-title&gt;

&lt;div style=&quot;grid-column: screen; text-align: center&quot;&gt;
  &lt;h1&gt;Impact of the Extrinsic Geometry on Linear Regression&lt;/h1&gt;
    
    &lt;img style=&quot;display: block; max-width: 15%; margin-top: 3rem; margin-bottom: 3rem; margin-left: auto; margin-right: auto; border-radius: 5%&quot; alt=&quot;Impact of the Extrinsic Geometry on Linear Regression&quot; src=&quot;https://tsairesearch.github.io//assets/images/extrinsic_geom/manifold_demo_3.pdf&quot; /&gt;
    
 &lt;/div&gt;
&lt;/d-title&gt;

&lt;d-article&gt;

&lt;p&gt;
In this paper, we study linear regression applied to data structured on a manifold. We assume that the data manifold is smooth and is embedded in a Euclidean space, and our objective is to reveal the impact of the data manifold's extrinsic geometry on the regression. Specifically, we analyze the impact of the manifold's curvatures (or higher order nonlinearity in the parameterization when the curvatures are locally zero) on the uniqueness of the regression solution. Our findings suggest that the corresponding linear regression does not have a unique solution when the manifold is flat. Otherwise, the manifold's curvature (or higher order nonlinearity in the embedding) may contribute significantly, particularly in the solution associated with the normal directions of the manifold. Our findings thus reveal the role of data manifold geometry in ensuring the stability of regression models for out-of-distribution inferences.
&lt;/p&gt;

&lt;h3&gt; Main results &lt;/h3&gt;
We investigate the local linear regression on smooth function $g(\mathbf{x})$ with data points sampled from different smooth manifolds $M\subset \mathbb R^d$.
&lt;p&gt;
&lt;ol&gt;
  &lt;li&gt; When $M$ is given by $M=(x, y=kx^2)\subset \mathbb R^2$, where $k$ characterizes the curvature, the linear regression has leading order solutions: $w_x = \frac{\partial g}{\partial x}$, $w_y = \frac{\partial g}{\partial y} + \frac{1}{2k}\frac{\partial^2 g}{\partial x^2}$. This implies a direct impact of the manifold's curvature on the linear regression. Numerical verifications of this result are provided below:

  &lt;div style=&quot;grid-column: screen &quot;&gt;

  &lt;figure&gt;
  &lt;div style=&quot;display: block; width: 100%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
  &lt;iframe width=&quot;400px&quot; height=&quot;290px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io/assets/images/extrinsic_geom/2d_regression.pdf&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
  &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
  Numerical simulations with different $k$.
  &lt;/div&gt;
  &lt;/figcaption&gt;
  &lt;/figure&gt;
  &lt;/div&gt;


&lt;/li&gt;
  &lt;li&gt; For any hypersurface $M\subset\mathbb R^d$, one can obtain a quadratic approximation locally. Then a generalized solution formula for the local linear regression can be explicitly obtained:
  $$ \begin{cases}
 w_{x_i} = \frac{\partial g}{\partial x_i},\\
	w_y = \frac{\partial g}{\partial y} + \frac{1}{2}\frac{\displaystyle\sum_{i=1}^{d-1}k_i \frac{\partial^2 g}{\partial x_i^2}}{\displaystyle\sum_{i=1}^{d-1}k_i^2}. \\
\end{cases} $$
&lt;/li&gt;

  &lt;li&gt;
  Data in practice usually contains noises. We show that a suitable scale of Gaussian noise $\sim\mathcal N(0, \sigma^2)$ can regularize the behavior caused by the problematic curvature. This balancing effect is demonstrated in the formula:
  $$ w_y = \frac{\partial g}{\partial y} + \frac{1}{2}\frac{k}{k^2+\frac{45}4\sigma^2}\frac{\partial^2 g}{\partial x^2},$$
  and verified in the numerical experiment:
  &lt;figure&gt;
  &lt;div style=&quot;display: block; width: 100%; margin-right: auto; margin-left: auto; text-align: center&quot;&gt;
  &lt;iframe width=&quot;400px&quot; height=&quot;290px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io/assets/images/extrinsic_geom/2d_regression_noise.pdf&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
  &lt;div style=&quot;display: block; margin-right: auto; margin-left: auto; width: 50%; text-align: center&quot;&gt;
  Numerical simulations with different scale of noise $\sigma$.
  &lt;/div&gt;
  &lt;/figcaption&gt;
  &lt;/figure&gt;
  The intuition behind is that the noise blurs the problematic structure of the manifold.
  &lt;/li&gt;

  &lt;li&gt; For manifold with codimension $&amp;gt;1$, we perform numerical experiments to demonstrate the importance of the awareness of extrinsic dimension and embedding dimension. These characteristics crucially affect the well-posedness of the regression algorithm. See paper for details.
  &lt;/li&gt;

&lt;/ol&gt;
&lt;/p&gt;

&lt;h3&gt; Publications &lt;/h3&gt;

&lt;p&gt;
&lt;li&gt; Liu, L., He, J., &amp;amp; Tsai, R. (2023) Linear Regression on Manifold Structured Data: the Impact of Extrinsic Geometry on Solutions. Topology, Algebra and Geometry in Machine Learning workshop, International Conference of Machine Learning
&lt;/li&gt;
&lt;/p&gt;


&lt;/d-article&gt;</content><author><name></name></author><category term="projects" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/extrinsic_geom/manifold_demo_3.pdf" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/extrinsic_geom/manifold_demo_3.pdf" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Previous Photos</title><link href="https://tsairesearch.github.io/photos" rel="alternate" type="text/html" title="Previous Photos" /><published>2023-02-20T00:00:00-06:00</published><updated>2023-02-20T00:00:00-06:00</updated><id>https://tsairesearch.github.io/photos</id><content type="html" xml:base="https://tsairesearch.github.io/photos">&lt;h3&gt; 2021 &lt;/h3&gt;
&lt;figure&gt;
&lt;img style=&quot;display: block; max-width: 100%; margin-top: 3rem; margin-bottom: 1rem; margin-left: auto; margin-right: auto&quot; alt=&quot;Previous Photos&quot; src=&quot;https://tsairesearch.github.io/assets/images/old_profile/group_photo-2021.png&quot; /&gt;
&lt;/figure&gt;

&lt;h3&gt; 2019 &lt;/h3&gt;
&lt;figure&gt;
&lt;img style=&quot;display: block; max-width: 100%; margin-top: 3rem; margin-bottom: 1rem; margin-left: auto; margin-right: auto&quot; alt=&quot;Previous Photos&quot; src=&quot;https://tsairesearch.github.io/assets/images/old_profile/group_photo-2019.jpg&quot; /&gt;
&lt;/figure&gt;</content><author><name></name></author><category term="photos" /><summary type="html">2021</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/github_avatar.jpg" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/github_avatar.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Numerical Wave Propagation Aided by Deep Learning</title><link href="https://tsairesearch.github.io/projects/wavelnn" rel="alternate" type="text/html" title="Numerical Wave Propagation Aided by Deep Learning" /><published>2021-08-16T00:00:00-05:00</published><updated>2021-08-16T00:00:00-05:00</updated><id>https://tsairesearch.github.io/projects/wavelnn</id><content type="html" xml:base="https://tsairesearch.github.io/projects/wavelnn">&lt;d-front-matter&gt;

  &lt;script type=&quot;text/json&quot;&gt;{
  &quot;authors&quot;: [
    {
      &quot;author&quot;: &quot;Hieu Nguyen&quot;,
      &quot;authorURL&quot;: &quot;https://dmi.unibas.ch/de/personen/hieu-huu-nguyen/&quot;,
      &quot;affiliation&quot;: &quot;Department of Mathematics and Computer Science, University of Basel&quot;,
      &quot;affiliationURL&quot;: &quot;https://dmi.unibas.ch/en/&quot;
    },
    {
      &quot;author&quot;: &quot;Yen-Hsi Richard Tsai&quot;,
      &quot;authorURL&quot;: &quot;https://www.ma.utexas.edu/users/ytsai/&quot;,
      &quot;affiliation&quot;: &quot;Department of Mathematics and Oden Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    }
  ]
  }&lt;/script&gt;

&lt;/d-front-matter&gt;

&lt;d-title&gt;
 &lt;div style=&quot;grid-column: screen; text-align: center&quot;&gt;
  &lt;h1&gt;Numerical Wave Propagation Aided by Deep Learning&lt;/h1&gt;
    
    &lt;img style=&quot;display: block; max-width: 15%; margin-top: 3rem; margin-bottom: 3rem; margin-left: auto; margin-right: auto; border-radius: 5%&quot; alt=&quot;Numerical Wave Propagation Aided by Deep Learning&quot; src=&quot;https://tsairesearch.github.io//assets/images/wavelnn/testvelmodels.png&quot; /&gt;
    
 &lt;/div&gt;


&lt;/d-title&gt;

&lt;d-article&gt;

&lt;p&gt; We propose a deep learning approach for wave propagation in media with multiscale wave speed, using a second-order linear wave equation model. We use neural networks to enhance the accuracy of a given inaccurate coarse solver, which under-resolves a class of multiscale wave media and wave fields of interest. Our approach involves generating training data by the given computationally efficient coarse solver and another sufficiently accurate solver, applied to a class of wave media (described by their wave speed profiles) and initial wave fields. We find that the trained neural networks can approximate the nonlinear dependence of the propagation on the wave speed as long as the causality is appropriately sampled in training data. We combine the neural-network-enhanced coarse solver with the parareal algorithm and demonstrate that the coupled approach improves the stability of parareal algorithms for wave propagation and improves the accuracy of the enhanced coarse solvers. &lt;/p&gt;


&lt;h3&gt; Simulation &lt;/h3&gt;


&lt;div style=&quot;column-grid: middle &quot;&gt;
&lt;figure&gt;
&lt;div style=&quot;display: block; width: 90%; text-align: center&quot;&gt;
&lt;iframe width=&quot;881px&quot; height=&quot;703px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io//assets/images/wavelnn/testvelmodels.png&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;figcaption&gt;

&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;/div&gt;

&lt;h3&gt; Training databases &lt;/h3&gt;
&lt;p&gt;
&lt;a href=&quot;https://utexas.box.com/s/av7cnf3shgwf9m42snrzfh49pyt9x9y0&quot;&gt; Click Here&lt;/a&gt; to get the training databases. &lt;/p&gt;




&lt;h3&gt; Publications &lt;/h3&gt;

&lt;p&gt;
&lt;li&gt; Nguyen, H., &amp;amp; Tsai, R. (2023). Numerical wave propagation aided by deep learning. Journal of Computational Physics, 475, 111828. &lt;/li&gt;
&lt;/p&gt;

&lt;!-- H. Nguyen and R. Tsai. Numerical Wave Propagation Aided by Deep Learning. ArXiv:2107.13184, 2021--&gt;

&lt;/d-article&gt;</content><author><name></name></author><category term="projects" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/wavelnn/testvelmodels.png" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/wavelnn/testvelmodels.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deep learning and the Low Dimensional Manifold Hypothesis</title><link href="https://tsairesearch.github.io/projects/lowdim" rel="alternate" type="text/html" title="Deep learning and the Low Dimensional Manifold Hypothesis" /><published>2021-06-30T00:00:00-05:00</published><updated>2021-06-30T00:00:00-05:00</updated><id>https://tsairesearch.github.io/projects/lowdim</id><content type="html" xml:base="https://tsairesearch.github.io/projects/lowdim">&lt;d-front-matter&gt;

  &lt;script type=&quot;text/json&quot;&gt;{
  &quot;authors&quot;: [
    {
      &quot;author&quot;: &quot;Juncai He&quot;,
      &quot;authorURL&quot;: &quot;https://juncaihe.github.io/&quot;,
      &quot;affiliation&quot;: &quot;Department of Mathematics, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.ma.utexas.edu/&quot;
    },
    {
      &quot;author&quot;: &quot;Yen-Hsi Richard Tsai&quot;,
      &quot;authorURL&quot;: &quot;https://www.ma.utexas.edu/users/ytsai/&quot;,
      &quot;affiliation&quot;: &quot;Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.ices.utexas.edu&quot;
    },
    {
      &quot;author&quot;: &quot;Rachel Ward&quot;,
      &quot;authorURL&quot;: &quot;https://sites.google.com/prod/view/rward&quot;,
      &quot;affiliation&quot;: &quot;Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.ices.utexas.edu&quot;
    }
  ]
  }&lt;/script&gt;
&lt;/d-front-matter&gt;

&lt;d-title&gt;

&lt;div style=&quot;grid-column: screen; text-align: center&quot;&gt;
  &lt;h1&gt;Deep learning and the Low Dimensional Manifold Hypothesis&lt;/h1&gt;
    
    &lt;img style=&quot;display: block; max-width: 15%; margin-top: 3rem; margin-bottom: 3rem; margin-left: auto; margin-right: auto; border-radius: 5%&quot; alt=&quot;Deep learning and the Low Dimensional Manifold Hypothesis&quot; src=&quot;https://tsairesearch.github.io//assets/images/lowdim/streamplot_sigma.png&quot; /&gt;
    
 &lt;/div&gt;
&lt;/d-title&gt;

&lt;d-article&gt;

&lt;p&gt;
The low dimensional manifold hypothesis posits that the data found in many applications, such as those involving natural images, lie (approximately) on low dimensional manifolds embedded in a high dimensional Euclidean space. In this setting, a typical neural network defines a function that takes a finite number of vectors in the embedding space as input.
However, one often needs to consider evaluating the optimized network at points outside the training distribution. This project considers the case in which the training data is distributed in a linear subspace of Rd. We derive estimates on the variation of the learning function, defined by a neural network, in the direction transversal to the subspace. We study the potential regularization effects associated with the network’s depth and noise in the codimension of the data manifold. We also present additional side effects in training due to the presence of noise.
&lt;/p&gt;
&lt;h3&gt; Main results &lt;/h3&gt;

&lt;p&gt;
&lt;ol&gt;
  &lt;li&gt;If the data points, including noise, lie on $M$, the linear network’s depth may
provide certain regularization or side effects. For ReLU neural networks, we prove that $\frac{\partial f}{\partial n_M}$ is sensitive to the initialization of a set of “untrainable” parameters.&lt;/li&gt;
  &lt;li&gt;If the noise has a small positive variance in the orthogonal complement of $M$,
then:
&lt;ul&gt;
  &lt;li&gt;$\frac{\partial f}{\partial n_M}$ can be made arbitrarily small, provided that the number of data points scales according to some inverse power of the variance for deep linear neural networks and nonlinear deep linear neural networks;&lt;/li&gt;
  &lt;li&gt;For linear neural network models, it may take a gradient descent algorithm exponentially (in the reciprocal of the variance) long to converge to the unique optimal model parameters, which yield small $\frac{\partial f}{\partial n_M}$. In addition, it may also need a long time to escape the near region of origin.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
  &lt;li&gt;The stability-accuracy trade-off. The role of noise can be interpreted as a stabilizer for a model when evaluated on points outside of the (clean) data distribution. However, adding noise to the data set will impact of the accuracy of the network's generalization error (for evaluation within the data distribution). For nonlinear data manifolds, uniform noise may render the labeled data incompatible.&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;
&lt;h3&gt; Publications &lt;/h3&gt;

&lt;p&gt;
&lt;li&gt; He, J., Tsai, R., &amp;amp; Ward, R. (2023). Side effects of learning from low-dimensional data embedded in a Euclidean space. Research in the Mathematical Sciences, 10(1), 13.
&lt;/li&gt;
&lt;/p&gt;
&lt;!--J. He, R. Tsai and R. Ward. &quot;Side-effects of Learning from Low Dimensional Data Embedded in an Euclidean Space&quot;. ArXiv: 2203.00614, 2022.
--&gt;

&lt;/d-article&gt;</content><author><name></name></author><category term="projects" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/lowdim/streamplot_sigma.png" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/lowdim/streamplot_sigma.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Selected Papers</title><link href="https://tsairesearch.github.io/papers" rel="alternate" type="text/html" title="Selected Papers" /><published>2019-11-09T00:00:00-06:00</published><updated>2019-11-09T00:00:00-06:00</updated><id>https://tsairesearch.github.io/papers</id><content type="html" xml:base="https://tsairesearch.github.io/papers">&lt;h3 id=&quot;exploration-and-surveillance&quot;&gt;Exploration and Surveillance&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Efficient and robust Sensor Placement in Complex Environments. Lukas Taus and Yen-Hsi Richard Tsai. Preprint 2023 &lt;a href=&quot;https://arxiv.org/abs/2309.08545&quot;&gt;[paper]&lt;/a&gt;&lt;a href=&quot;https://drive.google.com/file/d/12LGm468ZypvgAKlA1CVIV5Qu1ziiytb-/view&quot;&gt;[video]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Autonomous Exploration, Reconstruction, and Surveillance of 3d Environments aided by Deep Learning. Louis Ly and Yen-Hsi Richard Tsai. 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019.
&lt;a href=&quot;https://arxiv.org/abs/1809.06025&quot;&gt;[paper]&lt;/a&gt;
&lt;a href=&quot;http://helper.ipam.ucla.edu/publications/glws3/glws3_15672.pdf&quot;&gt;[slides]&lt;/a&gt;
&lt;a href=&quot;http://www.ipam.ucla.edu/abstract/?tid=15672&amp;amp;pcode=GLWS3&quot;&gt;[talk]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Strategy Synthesis for Surveillance-evasion Games with Learning-enabled Visibility Optimization. Suda Bharadwaj, Louis Ly, Bo Wu, Yen-Hsi Richard Tai, and Ufuk Topcu. 2019 Conference on Decisions and Control (CDC). IEEE, 2019.
&lt;a href=&quot;https://arxiv.org/abs/1911.07394&quot;&gt;[paper]&lt;/a&gt;
&lt;a href=&quot;http://helper.ipam.ucla.edu/publications/glws3/glws3_15672.pdf&quot;&gt;[slides]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An Efficient Algorithm for a Visibility-based Surveillance-evasion Game. Ryo Takei, Yen-Hsi Richard Tsai, Zhengyuan Zhou, and Yanina Landa. Communications in Mathematical Sciences 12.7 (2014): 1303-1327.
&lt;a href=&quot;https://www.intlpress.com/site/pub/pages/journals/items/cms/content/vols/0012/0007/a007/&quot;&gt;[paper]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A Practical Path-planning Algorithm for a Simple Car: a Hamilton-Jacobi Approach. Ryo Takei, Yen-Hsi Richard Tsai, Haochong Shen, and Yanina Landa. Proceedings of the 2010 American control conference. IEEE, 2010.
&lt;a href=&quot;ftp://ftp.math.ucla.edu/pub/camreport/cam09-74.pdf&quot;&gt;[paper]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;point-cloud-processing&quot;&gt;Point Cloud Processing&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Nearest Neighbor Sampling of Point Sets using Rays. Liangchen Liu, Louis Ly, Colin Macdonald, and Yen-Hsi Richard Tsai. Communications on Applied Mathematics and Computation 2023,
&lt;a href=&quot;https://arxiv.org/abs/1911.10737&quot;&gt;[paper]&lt;/a&gt;&lt;a href=&quot;https://drive.google.com/file/d/1U3imtMJiJrSDqYZ5knMYNCKo9vTbWbdZ/view?usp=share_link&quot;&gt;[poster]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;manifold-structured-data-and-its-impact-on-learning&quot;&gt;Manifold Structured Data and its Impact on Learning&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Linear Regression on Manifold Structured Data: the Impact of Extrinsic Geometry on Solutions. Liangchen Liu, Juncai He, and Richard Tsai. Topology, Algebra and Geometry in Machine Learning workshop, ICML2023 &lt;a href=&quot;https://arxiv.org/abs/2307.02478&quot;&gt;[paper]&lt;/a&gt;&lt;a href=&quot;https://icml.cc/media/PosterPDFs/ICML%202023/27589.png?t=1691533674.7285285&quot;&gt;[poster]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Side-effects of Learning from Low Dimensional Data Embedded in an Euclidean Space. Juncai He, Richard Tsai and Rachel Ward. Research in Mathematical Science 10.13 (2023).  &lt;a href=&quot;https://doi.org/10.1007/s40687-023-00378-y&quot;&gt;[paper]&lt;/a&gt;]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;source-localization&quot;&gt;Source Localization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Autonomous Source Discovery and Navigation in Complicated Environments. Yanina Landa,  Haochong Shen, Ryo Takei, Yen-Hsi Richard Tsai. Preprint 2009.
&lt;a href=&quot;ftp://ftp.math.ucla.edu/pub/camreport/cam09-73.pdf&quot;&gt;[paper]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Discovery of Point Sources in the Helmholtz Equation posed in Unknown Domains with Obstacles. Yanina Landa, Nicolay Tanushev, Yen-Hsi Richard Tsai. Communications in Mathematical Sciences 9.3 (2011): 903-928.
&lt;a href=&quot;https://www.intlpress.com/site/pub/pages/journals/items/cms/content/vols/0009/0003/a011/&quot;&gt;[paper]&lt;/a&gt;
&lt;a href=&quot;https://www.dropbox.com/s/h3pesvk99rom90r/inverse_source_problem_nonlinear-helmholtz.pdf?dl=0&quot;&gt;[slides]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Point Source Identification in Nonlinear Advection–Diffusion–Reaction Systems. Alexander Mamonov and Yen-Hsi Richard Tsai. Inverse Problems 29.3 (2013): 035009.
&lt;a href=&quot;ftp://ftp.math.ucla.edu/pub/camreport/cam12-15.pdf&quot;&gt;[paper]&lt;/a&gt;
&lt;a href=&quot;https://www.dropbox.com/s/mnk5a3j3xiozh43/srcid-talk-richard.pdf?dl=0&quot;&gt;[slides]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heat Source Identification based on Constrained Minimization. Yingying Li, Stanley Osher, and Yen-Hsi Richard Tsai. Inverse Problems and Imaging 8.1 (2014): 199-221.
&lt;a href=&quot;https://www.researchgate.net/profile/Richard_Tsai3/publication/264998169_Heat_source_identification_based_on_L1_constrained_minimization/links/54cbafab0cf24601c088b3d8/Heat-source-identification-based-on-L1-constrained-minimization.pdf&quot;&gt;[paper]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;multiscale-coupling-algorithms&quot;&gt;Multiscale Coupling Algorithms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Stabilization of parareal algorithms for long time computation of a class of highly oscillatory Hamiltonian flows using data. Preprint (2023) &lt;a href=&quot;https://arxiv.org/pdf/2309.01225.pdf&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;A Stable Parareal-like Method for the Second Order Wave Equation. Hieu Nguyen and Richard Tsai. Journal of Computational Physics (2020). Accepted.
&lt;a href=&quot;https://doi.org/10.1016/j.jcp.2019.109156&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--
### Deep Neural Networks and Finite Element Methods

- ReLU deep neural networks from the hierarchical basis perspective. Juncai He, Lin Li and Jinchao Xu. Preprint 2021. [[paper]](https://arxiv.org/abs/2105.04156)
- ReLU deep neural networks and linear finite elements. Juncai He, Lin Li, Jinchao Xu and Chunyue Zheng. Journal of Computational Mathematics 38.3 (2020): 502-527. [[paper]](https://www.global-sci.org/intro/article_detail/jcm/15798.html)
- Power series expansion neural network. Qipin Chen, Wenrui Hao and Juncai He. Preprint 2021. [[paper]](https://arxiv.org/abs/2102.13221)

### Convolutional neural networks and multigrid

- Constrained Linear Data-feature Mapping in Image Classification. Juncai He, Yuyan Chen, Lian Zhang and Jinchao Xu. Preprint 2020. [[paper]](https://arxiv.org/abs/1911.10428)
- MgNet: a unified framework of multigrid and convolutional neural network. Juncai He and Jinchao Xu.  Science China Mathematics 62.7 (2019): 1331–1354. [[paper]](https://link.springer.com/article/10.1007%2Fs11425-019-9547-2)

### Training algorithms in deep learning

-  Make ![ell_1](https://juncaihe.github.io/eqs/2349274112114421517-130.png) regularization effective in training sparse CNN. Juncai He, Xiaodong Jia, Jinchao Xu, Liang Zhao and Lian Zhang. Computational Optimization and Applications (2020). [[paper]](https://link.springer.com/article/10.1007/s10589-020-00202-1) --&gt;</content><author><name></name></author><category term="papers" /><summary type="html">Exploration and Surveillance Efficient and robust Sensor Placement in Complex Environments. Lukas Taus and Yen-Hsi Richard Tsai. Preprint 2023 [paper][video]</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/github_avatar.jpg" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/github_avatar.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Mathematics in Deep Learning —Syllabus</title><link href="https://tsairesearch.github.io/notes/syllabus" rel="alternate" type="text/html" title="Mathematics in Deep Learning —Syllabus" /><published>2019-10-20T00:00:00-05:00</published><updated>2019-10-20T00:00:00-05:00</updated><id>https://tsairesearch.github.io/notes/syllabus</id><content type="html" xml:base="https://tsairesearch.github.io/notes/syllabus">&lt;p&gt;Welcome to the zoo!&lt;/p&gt;

&lt;p&gt;A tentative list of topics to be covered in this course.&lt;/p&gt;

&lt;p&gt;The course will be conducted with a mixture of regular lectures, seminar style presentation and discussion.&lt;/p&gt;

&lt;p&gt;Participants of this course are expected to present certain relevant concepts from suggested reading assignments, and arrange the presentation in a certain uniform style.&lt;/p&gt;

&lt;h3 id=&quot;deep-learning-architectures-and-the-related-applications&quot;&gt;Deep learning architectures and the related applications:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;basic multi-layer neural networks (NNs)&lt;/li&gt;
  &lt;li&gt;convolutional Neural networks (CNNs)&lt;/li&gt;
  &lt;li&gt;residual neural networks (ResNets)&lt;/li&gt;
  &lt;li&gt;generative adversarial networks (GANs) and the Wasserstein GANs&lt;/li&gt;
  &lt;li&gt;recurrent neural network (RNN)&lt;/li&gt;
  &lt;li&gt;LSTMs&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;convolutional Neural networks for graphs&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;encoder-decoders&lt;/li&gt;
  &lt;li&gt;reinforcement learning (value iteration, policy iteration)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;algorithmic-components&quot;&gt;Algorithmic components:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;stochastic gradient descent algorithms&lt;/li&gt;
  &lt;li&gt;backpropagation and automatic differentiation&lt;/li&gt;
  &lt;li&gt;computational graphs&lt;/li&gt;
  &lt;li&gt;search algorithms: Monte-Carlo Tree Search used in AlphaGo&lt;/li&gt;
  &lt;li&gt;classical multi-level algorithms: multigrid methods&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;approximation-theory&quot;&gt;Approximation theory:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;classical multi-resolution analysis (wavelet)&lt;/li&gt;
  &lt;li&gt;compressive sensing&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The curse of dimensionality!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;single layer universal approximation theory&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;multi-layer neural network approximation theory&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;over-parameterization and generalization&lt;/li&gt;
  &lt;li&gt;linear algebra: defining notions of distances from data matrices&lt;/li&gt;
  &lt;li&gt;manifold learning&lt;/li&gt;
  &lt;li&gt;transfer learning&lt;/li&gt;
  &lt;li&gt;adversarial attacks&lt;/li&gt;
  &lt;li&gt;differential privacy&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;random-graphs-and-random-matrices&quot;&gt;Random graphs and random matrices:&lt;/h3&gt;

&lt;h3 id=&quot;optimization-algorithms-and-theories&quot;&gt;Optimization algorithms and theories:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;efficient algorithms for finding a/the minimum of a loss function&lt;/li&gt;
  &lt;li&gt;mini-batch and minimizing variances&lt;/li&gt;
  &lt;li&gt;duality, saddle point problems&lt;/li&gt;
  &lt;li&gt;primal-dual type splitting algorithms&lt;/li&gt;
  &lt;li&gt;Nesterov’s algorithm&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;how to select your mini-batches?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;which loss function?&lt;/li&gt;
  &lt;li&gt;the vanishing gradient problem of using &lt;em&gt;Sigmoids&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;cross-entropy&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;training-issues&quot;&gt;Training issues&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The problem with Sigmoids: Sigmoid “saturates” by approaching 1, as the input increases. (ReLU just keeps increasing)  When Sigmoid(x) is very close to 1, it’s gradient is very close to 0, and gives little information for gradient descent algorithms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The dying ReLu problem: ReLU neurons becomes inactive (only outputs zero regardless of inputs. Little theoretical results about this phen
 omenon).
    &lt;ul&gt;
      &lt;li&gt;Remedy type 1: modify the network architecture, and possibly replace the activation function&lt;/li&gt;
      &lt;li&gt;Remedy type 2: introduce additional training steps – normalization techniques and introducing “dropouts”&lt;/li&gt;
      &lt;li&gt;Remedy type 3: initialization&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;batch normalization (Cf. Ioffe and Szegedy, 2015): It is a technique that inserts layers into the deep neural network that transform the output for the batch to be zero mean unit variance.&lt;/li&gt;
  &lt;li&gt;weight initialization: carefully choose random initial weights with suitable variances (Cf: Hanin and Rolnick, How to start training: the effect of initialization and architecture)&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;initialization-for-training-neural-networks&quot;&gt;Initialization for training neural networks&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;For ReLU networks: Karniadakis proposes randomize asymmetric initialization&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dynamical-system&quot;&gt;Dynamical system:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;stability&lt;/li&gt;
  &lt;li&gt;automatic step size control&lt;/li&gt;
  &lt;li&gt;optimal control of dynamical systems&lt;/li&gt;
  &lt;li&gt;Decoupling algorithms&lt;/li&gt;
  &lt;li&gt;Impulse method&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimal-transport-theory-and-algorithms&quot;&gt;Optimal transport theory and algorithms:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Comparing probability densities&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Earth mover’s distances “convexify”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The Benamou-Brenier fluid formulation and related algorithms&lt;/li&gt;
  &lt;li&gt;Sinkhorn algorithm&lt;/li&gt;
  &lt;li&gt;Information Geometry&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mean-field-games&quot;&gt;Mean field games&lt;/h3&gt;

&lt;h3 id=&quot;some-novel-applications&quot;&gt;Some novel applications:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Beyond image processing and facial recognition&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;deep-learning-for-scientific-computing&quot;&gt;Deep learning for scientific computing:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;wave propagation&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Louis and Richard</name></author><category term="notes" /><summary type="html">Welcome to the zoo!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/github_avatar.jpg" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/github_avatar.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Multiscale coupling algorithms using parareal style structures</title><link href="https://tsairesearch.github.io/projects/parareal" rel="alternate" type="text/html" title="Multiscale coupling algorithms using parareal style structures" /><published>2019-10-04T00:00:00-05:00</published><updated>2019-10-04T00:00:00-05:00</updated><id>https://tsairesearch.github.io/projects/parareal</id><content type="html" xml:base="https://tsairesearch.github.io/projects/parareal">&lt;d-front-matter&gt;
  &lt;script type=&quot;text/json&quot;&gt;{
  &quot;authors&quot;: [
    {
      &quot;author&quot;: &quot;Hieu Nguyen&quot;,
      &quot;authorURL&quot;: &quot;https://www.oden.utexas.edu/&quot;,
      &quot;affiliation&quot;: &quot;Oden Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    },
    {
      &quot;author&quot;: &quot;Yen-Hsi Richard Tsai&quot;,
      &quot;authorURL&quot;: &quot;https://www.ma.utexas.edu/users/ytsai/&quot;,
      &quot;affiliation&quot;: &quot;Oden Institute for Computational Engineering and Sciences, University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    }
  ]
  }&lt;/script&gt;
&lt;/d-front-matter&gt;

&lt;d-title&gt;
 &lt;div style=&quot;grid-column: screen; text-align: center&quot;&gt;
  &lt;h1&gt;Multiscale coupling algorithms using parareal style structures&lt;/h1&gt;
    
    &lt;img style=&quot;display: block; max-width: 15%; margin-top: 3rem; margin-bottom: 3rem; margin-left: auto; margin-right: auto; border-radius: 5%&quot; alt=&quot;Multiscale coupling algorithms using parareal style structures&quot; src=&quot;https://tsairesearch.github.io//assets/images/wave/waveinmarmousi.gif&quot; /&gt;
    
 &lt;/div&gt;


&lt;/d-title&gt;

&lt;d-article&gt;

&lt;p&gt;We are developing a data-driven parallel-in-time iterative method to solve the homogeneous second-order wave equation. The new method involves a coarse-scale propagator and a fine-scale propagator which fully resolves the medium using finer spatial grid and shorter time steps. The fine-scale propagator is run in parallel for short time subintervals. The two propagators are coupled in an iterative way similar to the standard parareal method. We train a Neural Network, which structure mimics the physics of wave propagation, to enhance the accuracy of the coarse-scale propagator such that the parareal iteration stabilizes and hence converges.&lt;/p&gt;

&lt;h3&gt; Simulation &lt;/h3&gt;


&lt;div style=&quot;column-grid: middle &quot;&gt;

&lt;figure&gt;
&lt;div style=&quot;display: block; width: 90%; text-align: center&quot;&gt;
&lt;iframe width=&quot;547px&quot; height=&quot;547px&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://tsairesearch.github.io/assets/images/wave/waveinmarmousi.gif&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;figcaption&gt;
Wave propagating through Marmousi velocity profile.
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;/div&gt;


&lt;h3&gt; Publications &lt;/h3&gt;

&lt;p&gt;
&lt;li&gt; H. Nguyen, and R. Tsai. &quot;A stable parareal-like method for the second order wave equation.&quot; Journal of Computational Physics (2020). Accepted.&lt;/li&gt;
&lt;li&gt; G. Ariel, H. Nguyen, and R. Tsai. &quot;\theta-parareal schemes.&quot; arXiv preprint arXiv:1704.06882 (2017). &lt;/li&gt;
&lt;/p&gt;



&lt;/d-article&gt;</content><author><name></name></author><category term="projects" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/wave/waveinmarmousi.gif" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/wave/waveinmarmousi.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Strategy Synthesis for Surveillance-Evasion Games with Learning-enabled Visibility Optimization</title><link href="https://tsairesearch.github.io/projects/evasion" rel="alternate" type="text/html" title="Strategy Synthesis for Surveillance-Evasion Games with Learning-enabled Visibility Optimization" /><published>2019-03-20T00:00:00-05:00</published><updated>2019-03-20T00:00:00-05:00</updated><id>https://tsairesearch.github.io/projects/evasion</id><content type="html" xml:base="https://tsairesearch.github.io/projects/evasion">&lt;d-front-matter&gt;
  &lt;script type=&quot;text/json&quot;&gt;{
  &quot;authors&quot;: [
    {
      &quot;author&quot;: &quot;Suda Bharadwaj&quot;,
      &quot;authorURL&quot;: &quot;https://www.linkedin.com/in/suda-bharadwaj-23784137&quot;,
      &quot;affiliation&quot;: &quot;Aerospace Engineering &amp; Engineering Mechanics, The University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.robotics.utexas.edu&quot;
    },
    {
      &quot;author&quot;: &quot;Louis Ly&quot;,
      &quot;authorURL&quot;: &quot;https://longlouisly.github.io/&quot;,
      &quot;affiliation&quot;: &quot;Institute for Computational Engineering and Sciences, The University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    },
    {
      &quot;author&quot;: &quot;Bo Wu&quot;,
      &quot;authorURL&quot;: &quot;https://users.oden.utexas.edu/~bwu/&quot;,
      &quot;affiliation&quot;: &quot;Institute for Computational Engineering and Sciences, The University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    },
    {
      &quot;author&quot;: &quot;Yen-Hsi Richard Tsai&quot;,
      &quot;authorURL&quot;: &quot;https://www.ma.utexas.edu/users/ytsai/&quot;,
      &quot;affiliation&quot;: &quot;Institute for Computational Engineering and Sciences, The University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    },
    {
      &quot;author&quot;: &quot;Ufuk Topcu&quot;,
      &quot;authorURL&quot;: &quot;https://www.ae.utexas.edu/facultysites/topcu/wiki/index.php/Main_Page&quot;,
      &quot;affiliation&quot;: &quot;Aerospace Engineering and Engineering Mechanics, The University of Texas at Austin&quot;,
      &quot;affiliationURL&quot;: &quot;https://www.oden.utexas.edu&quot;
    }
  ]
  }&lt;/script&gt;
&lt;/d-front-matter&gt;

&lt;d-title&gt;
 &lt;div style=&quot;grid-column: screen; text-align: center&quot;&gt;
  &lt;h1&gt;Strategy Synthesis for Surveillance-Evasion Games with Learning-enabled Visibility Optimization&lt;/h1&gt;
    
    &lt;img style=&quot;display: block; max-width: 15%; margin-top: 3rem; margin-bottom: 3rem; margin-left: auto; margin-right: auto; border-radius: 5%&quot; alt=&quot;Strategy Synthesis for Surveillance-Evasion Games with Learning-enabled Visibility Optimization&quot; src=&quot;https://tsairesearch.github.io//assets/images/evasion/patrol.gif&quot; /&gt;
    
 &lt;/div&gt;


&lt;/d-title&gt;

&lt;d-article&gt;

&lt;p&gt;We study a two-player game with a quantitative surveillance requirement on an
adversarial target moving in a discrete state space and a secondary objective
to maximize short-term visibility of the environment. We impose the
surveillance requirement as a temporal logic constraint. We then use a greedy
approach to determine vantage points that optimize a notion of information
gain, namely, the number of newly-seen states. By using a convolutional neural
network trained on a class of environments, we can efficiently approximate
the information gain at each potential vantage point. Subsequent vantage points
are chosen such that moving to that location will not jeopardize the
surveillance requirement, regardless of any future action chosen by the
target. Our method combines guarantees of correctness from formal methods with
the scalability of machine learning to provide an efficient approach for
surveillance-constrained visibility optimization.&lt;/p&gt;

&lt;h3&gt; Simulations &lt;/h3&gt;


&lt;div style=&quot;grid-column: text;&quot;&gt;

&lt;figure&gt;
&lt;div style=&quot;display: inline; float: left; width: 50%; text-align: center&quot;&gt;
&lt;iframe frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://drive.google.com/file/d/1Is8AIsAQB5zL1iCdtaKLMpL33oYpqV99/preview&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;div style=&quot;display: inline; float: left; width: 50%; text-align: center&quot;&gt;
&lt;iframe frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; src=&quot;https://drive.google.com/file/d/1Ppf2gh0pmyD7lAZO9WCxh04ln7RaUEtX/preview&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;figcaption&gt;
We present videos demonstrating the agent's strategies synthesized by our
algorithm.  The blue circle corresponds to the controlled agent and the orange
circle corresponds to the hostile target.  Red cells are obstacles that cannot
be passed through and obscure vision. Black cells correspond to states
the agent cannot see.

The video on the left shows an agent with the surveillance
objective: always mantain visibility of the target. Notice the agent tends to stay still until it is necessary to move.
On the right video, the agent must also patrol the environment, in addition to the surveillance requirement.
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;/div&gt;


&lt;h3&gt; Publications &lt;/h3&gt;

&lt;p&gt;
&lt;li&gt; Suda Bharadwaj, Louis Ly, Bo Wu, Richard Tsai, and Ufuk Topcu. &quot;Strategy synthesis for surveillance-evasion games with learning-enabled visibility optimization.&quot; 2019 Conference on Decision and Control (CDC). IEEE, 2019. &lt;/li&gt;
&lt;/p&gt;



&lt;/d-article&gt;</content><author><name></name></author><category term="projects" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://tsairesearch.github.io/assets/images/evasion/patrol.gif" /><media:content medium="image" url="https://tsairesearch.github.io/assets/images/evasion/patrol.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>